{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Using Doc2Vec to calculate similarity between documents\n",
    "\n",
    "## Overview\n",
    "Duration: 1min\n",
    "\n",
    "This tutorial will show you how to use Doc2Vec to calculate similarity scores between pairs of documents. Doc2Vec is an NLP tool for representing documents as a vector and is a generalizing of the Word2Vec method. \n",
    "\n",
    "Before you start, follow the instructions from [README.md](https://github.com/ucals/bettertogether/blob/master/README.md) to get the PDF documents in your local machine. This corpus, which will be located in `pdfs/` folder, contains ~1,000 documents. They are essays written by OMSCS CS-6460 students in Fall 2018 class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What you'll learn\n",
    "\n",
    "- How to pre-process text, getting it ready to apply NLP tecniques, by using [Gensim Library](https://radimrehurek.com/gensim/)\n",
    "- How to train Doc2Vec model, also using [Gensim Library](https://radimrehurek.com/gensim/)\n",
    "- How to test it by eye, using [TextRazor](https://www.textrazor.com/) classification API to check if the similarities from our trained model are actually true\n",
    "\n",
    "### What you'll need\n",
    "\n",
    "- The corpora: a collection of PDF files correspondent to Assignments 2, 3 and 4 from OMSCS CS-6460 students\n",
    "- Some basic Python knowledge\n",
    "\n",
    "### Steps:\n",
    "\n",
    "1. Preprocess the text: 10min\n",
    "2. Train Doc2Vec model: 5min\n",
    "3. Test the result: 10min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Preprocess the text\n",
    "\n",
    "The first step will be preprocess the text, getting it ready to train Doc2Vec model. We will do this by combining all assignment documents in one single file called `unigram_sentences_all_assignment_X.txt`, where X will be 2, 3 or 4 depending on the assignment. In this file, each line will be one sentence from the assignment documents. We will read the assignment documents line-by-line, pre-process each line using a simple gensim pre-processing tool (i.e., tokenize text into individual words, remove punctuation, set to lowercase, etc), and return a list of words. Then, we will combine the list of words of each line in one string (separated by spaces) and store it in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic setup\n",
    "import logging, gensim, os, textract\n",
    "import itertools as it\n",
    "from gensim.parsing.preprocessing import preprocess_string, remove_stopwords\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "assignment = \"Assignment 1\"\n",
    "save_files = True\n",
    "\n",
    "assignment_code = assignment.lower().replace(' ', '_')\n",
    "path = os.path.join(os.getcwd(), \"pdfs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_sentences_filepath = os.path.join(path, 'unigram_sentences_all_' + assignment_code + '.txt')\n",
    "\n",
    "if save_files:\n",
    "    with open(unigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "        for file in os.listdir(path):\n",
    "            if assignment in str(file):\n",
    "                text = textract.process(os.path.join(path, file)).decode(\"utf-8\")\n",
    "                unigram_sentences = []\n",
    "                for sent in gensim.summarization.textcleaner.get_sentences(text):\n",
    "                    processed = preprocess_string(sent)\n",
    "                    if len(processed) > 0:\n",
    "                        f.write(\" \".join(processed) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim's LineSentence class provides a convenient iterator for working with other gensim components. It streams the documents/sentences from disk, so that you never have to hold the entire corpus in RAM at once. This allows you to scale your modeling pipeline up to potentially very large corpora:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_sentences = gensim.models.word2vec.LineSentence(unigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to apply *phrase modeling* to combine tokens that together represent meaningful multi-word concepts (bi-grams and tri-grams). After applying it, `new york` would become `new_york`; `new york times` would become `new_york_times`. \n",
    "\n",
    "First, let's create bi-grams from the uni-grams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-06 11:37:38,715 : INFO : collecting all words and their counts\n",
      "2018-11-06 11:37:38,718 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2018-11-06 11:37:38,847 : INFO : PROGRESS: at sentence #10000, processed 46813 words and 32555 word types\n",
      "2018-11-06 11:37:38,888 : INFO : collected 42725 word types from a corpus of 64536 words (unigram + bigrams) and 13742 sentences\n",
      "2018-11-06 11:37:38,888 : INFO : using 42725 counts as vocab in Phrases<0 vocab, min_count=5, threshold=10.0, max_vocab_size=40000000>\n",
      "2018-11-06 11:37:38,889 : INFO : saving Phrases object under /Users/carlossouza/Dropbox/2018/OMSCS/CS-6460-Education-Technology/bettertogether/pdfs/bigram_model_all_assignment_1, separately None\n",
      "2018-11-06 11:37:38,950 : INFO : saved /Users/carlossouza/Dropbox/2018/OMSCS/CS-6460-Education-Technology/bettertogether/pdfs/bigram_model_all_assignment_1\n"
     ]
    }
   ],
   "source": [
    "bigram_model_filepath = os.path.join(path, 'bigram_model_all_' + assignment_code)\n",
    "bigram_model = gensim.models.Phrases(unigram_sentences)\n",
    "if save_files:\n",
    "    bigram_model.save(bigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "bigram_sentences_filepath = os.path.join(path, 'bigram_sentences_all_' + assignment_code + '.txt')\n",
    "\n",
    "if save_files:\n",
    "    with open(bigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "        for unigram_sentence in unigram_sentences:\n",
    "            bigram_sentence = \" \".join(bigram_model[unigram_sentence])\n",
    "            f.write(bigram_sentence + \"\\n\")\n",
    "            \n",
    "bigram_sentences = gensim.models.word2vec.LineSentence(bigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that it worked by examining a small slice of our file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "student interview\n",
      "\n",
      "medic profession gain insight method effect futur\n",
      "\n",
      "direct interest\n",
      "\n",
      "student conclud human patient simul\n",
      "\n",
      "prefer technolog advanc medic educ physician provid interview\n",
      "\n",
      "student ad color gener consensu medic commun simul\n",
      "\n",
      "supplement replac true human human interact medic educ cadav\n",
      "\n",
      "dissect virtual_realiti inform real_life\n",
      "\n",
      "project interest recent ask compani work\n",
      "\n",
      "“doximity” profession platform physician lower skill healthcar worker rn\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bigram_sentences = gensim.models.word2vec.LineSentence(bigram_sentences_filepath)\n",
    "\n",
    "for bigram_sentence in it.islice(bigram_sentences, 3000, 3010):\n",
    "    print(u' '.join(bigram_sentence))\n",
    "    print(u'')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the algorithm combined some tokens, forming bi-grams like `mobil_devic`, `social_media`, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use the bi-grams to create tri-grams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-06 11:37:49,378 : INFO : collecting all words and their counts\n",
      "2018-11-06 11:37:49,379 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2018-11-06 11:37:49,499 : INFO : PROGRESS: at sentence #10000, processed 44481 words and 32970 word types\n",
      "2018-11-06 11:37:49,547 : INFO : collected 43344 word types from a corpus of 61388 words (unigram + bigrams) and 13742 sentences\n",
      "2018-11-06 11:37:49,547 : INFO : using 43344 counts as vocab in Phrases<0 vocab, min_count=5, threshold=10.0, max_vocab_size=40000000>\n",
      "2018-11-06 11:37:49,548 : INFO : saving Phrases object under /Users/carlossouza/Dropbox/2018/OMSCS/CS-6460-Education-Technology/bettertogether/pdfs/trigram_model_all_assignment_1, separately None\n",
      "2018-11-06 11:37:49,618 : INFO : saved /Users/carlossouza/Dropbox/2018/OMSCS/CS-6460-Education-Technology/bettertogether/pdfs/trigram_model_all_assignment_1\n"
     ]
    }
   ],
   "source": [
    "trigram_model_filepath = os.path.join(path, 'trigram_model_all_' + assignment_code)\n",
    "trigram_model = gensim.models.Phrases(bigram_sentences)\n",
    "if save_files:\n",
    "    trigram_model.save(trigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "trigram_sentences_filepath = os.path.join(path, 'trigram_sentences_all_' + assignment_code + '.txt')\n",
    "\n",
    "if save_files:\n",
    "    with open(trigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "        for bigram_sentence in bigram_sentences:\n",
    "            trigram_sentence = \" \".join(trigram_model[bigram_sentence])\n",
    "            f.write(trigram_sentence + \"\\n\")\n",
    "            \n",
    "trigram_sentences = gensim.models.word2vec.LineSentence(trigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "trigram_assignments_filepath = os.path.join(path, 'trigram_transformed_assignments_all_' + assignment_code + '.txt')\n",
    "\n",
    "if save_files:\n",
    "    with open(trigram_assignments_filepath, 'w', encoding='utf_8') as f:\n",
    "        for file in os.listdir(path):\n",
    "            if assignment in str(file):\n",
    "                text = textract.process(os.path.join(path, file)).decode(\"utf-8\")\n",
    "                unigram_assignment = preprocess_string(text)\n",
    "                bigram_assignment = bigram_model[unigram_assignment]\n",
    "                trigram_assignment = trigram_model[bigram_assignment]\n",
    "                f.write(\" \".join(trigram_assignment) + \"\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine a slice of our tri-gram file to check that it worked as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explor multipl type\n",
      "\n",
      "game competit strategi consid audienc modifi depth\n",
      "\n",
      "content order cover benefici\n",
      "\n",
      "audienc\n",
      "\n",
      "program experi didn’t want basic\n",
      "\n",
      "creat tool\n",
      "\n",
      "gamifi learn program python includ leaderboard user\n",
      "\n",
      "sens accomplish encourag learn\n",
      "\n",
      "like cool\n",
      "\n",
      "project took account audienc like fun wai learn python\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for trigram_sentence in it.islice(trigram_sentences, 1000, 1010):\n",
    "    print(u' '.join(trigram_sentence))\n",
    "    print(u'')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it worked: our algorithm created tri-grams like `simul_base_learn` (simulated based learning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the original version with the preprocessed version of an assignment\n",
    "\n",
    "Let's see exactly what we did by randomly selecting an assignment and comparing as slice (first 2000 characters) of its original version with an slice of its preprocessed version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "\n",
    "file_list = []\n",
    "for file in os.listdir(path):\n",
    "    if assignment in str(file):\n",
    "        file_list.append(file)\n",
    "\n",
    "random_index = randrange(len(file_list))\n",
    "random_filename = file_list[random_index]\n",
    "\n",
    "with open(trigram_assignments_filepath) as f:\n",
    "    trigram_assignments_list = f.readlines()\n",
    "\n",
    "trigram_assignments_list = [x.strip() for x in trigram_assignments_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HW1\n",
      "Luke Rucks\n",
      "John Yahiro tackled the problem of ghost writing, creating a web app that can identify\n",
      "assignments written by someone other than the person who claimed to have wrote it. He used\n",
      "cosine similarity and Dale-Chall readability in combination for this task. Cosign similarity\n",
      "compares the relative frequency of terms appearing in a given writing sample, whereas DaleChall measures the frequency of use of words outside the most common 3000 words used by 4th\n",
      "graders. He ended up having a false-positive ghost-written identification rate of 2.7%, while his\n",
      "thresholds were chosen so that there would be no false negatives. The biggest question regards\n",
      "his data: there were only 3 students who had written 3 papers, and because of the general lack of\n",
      "data, there was no test-train split, so his results (though impressive) were almost certainly\n",
      "massively overfit to the limited data. As a follow-up to this, I would like to see both an\n",
      "application that could do more and one which is more robust. First, any actually statistically\n",
      "reliable solution will need more data than what was provided to John. For an addition to the\n",
      "application, I would love to see a classification system that would allow you to see a probability\n",
      "distribution on who may have written a given paper based on a catalog of previously completed\n",
      "ones. This could help catch not only the student shirking work but the person helping them\n",
      "cheat.\n",
      "David Murphy uses adaptive learning to teach music theory, training students to learn\n",
      "things like 3rds, 5ths, chords, and progressions while increasing the difficulty relative to what the\n",
      "student is capable of doing. On an iPhone app, students must identify chord types and harmony\n",
      "distances based on sound and sight; the app increases difficulty by increasing tempo, tempo\n",
      "variance, and base note variance based on the student’s displayed mastery at correctly answering\n",
      "previous questions on the subject. One thing that I’d like to see in a follow-up to enhance the\n",
      "app’s usef\n"
     ]
    }
   ],
   "source": [
    "original = textract.process(os.path.join(path, random_filename)).decode(\"utf-8\")\n",
    "print(original[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "luke ruck john yahiro tackl problem ghost write creat web_app identifi assign written person claim wrote cosin similar dale chall readabl combin task cosign similar compar rel frequenc term appear given write sampl dalechal measur frequenc us word outsid common word grader end have fals posit ghost written identif rate threshold chosen fals neg biggest question regard data student written paper gener lack data test train split result impress certainli massiv overfit limit data follow like applic robust actual statist reliabl solut need data provid john addit applic love classif allow probabl distribut written given paper base catalog previous complet on help catch student shirk work person help cheat david_murphi us adapt_learn teach music_theori train student learn thing like rd th chord progress increas difficulti rel student capabl iphon app student identifi chord type harmoni distanc base sound sight app increas difficulti increas tempo tempo varianc base note varianc base student’ displai masteri correctli answer previou question subject thing i’d_like follow enhanc app’ us set vocalist preform given harmoni request instead mere identifi essenc current app teach read listen propos extens teach speak biggest drawback extens current app work beautifulli phone tuner notori don’t extens requir pitch recognit phone tuner srilaxmi_komanduri research paper look mooc help femal represent stem us data introductori scienc cours edx find larg differ enrol cours men_women far men choos enrol statist differ perform enrol consist research i’m awar subject gener find gender_gap stem_field entir person prefer individu caus group differ enrol oppos perform enrol system bia prevent enrol interest bit follow research subject caus especi regard scienc engin larg gap like medic school graduat nearli reach pariti role biologi plai caus differ interest oppos like gener human reluct larg outnumb group refer luke ruck yahiro john ghost write detector detect ghost write plagiar short es\n"
     ]
    }
   ],
   "source": [
    "processed = trigram_assignments_list[random_index]\n",
    "print(processed[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, our algorithm worked perfectly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Train Doc2Vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we train our model, let's define a function `read_corpus` to open the train/test file (with latin encoding), read the file line-by-line, pre-process each line using a simple gensim pre-processing tool (i.e., tokenize text into individual words, remove punctuation, set to lowercase, etc), and return a list of words. Also, to train the model, we'll need to associate a tag/number with each line of the training corpus. In our case, the tag is the student's name, given by `get_student_name` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(fname, tokens_only=False):\n",
    "    with open(fname, encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if tokens_only:\n",
    "                yield gensim.utils.simple_preprocess(line)\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), [get_student_name(i)])\n",
    "                \n",
    "def get_student_name(file_index):\n",
    "    return file_list[file_index].split(\" -\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = list(read_corpus(trigram_assignments_filepath))\n",
    "test_corpus = list(read_corpus(trigram_assignments_filepath, tokens_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll instantiate a Doc2Vec model with a vector size with 300 words and iterating over the training corpus 100 times. We set the minimum word count to 5 in order to discard words with very few occurrences. (Without a variety of representative examples, retaining such infrequent words can often make a model worse!) Typical iteration counts in published 'Paragraph Vectors' results, using 10s-of-thousands to millions of docs, are 10-20. More iterations take more time and eventually reach a point of diminishing returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=300, min_count=5, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's build a vocabulary. Essentially, the vocabulary is a dictionary (accessible via `model.wv.vocab`) of all of the unique words extracted from the training corpus along with the count (e.g., `model.wv.vocab['penalty'].count` for counts for the word `penalty`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-06 11:38:30,942 : INFO : collecting all words and their counts\n",
      "2018-11-06 11:38:30,943 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2018-11-06 11:38:30,958 : INFO : collected 5569 word types and 180 unique tags from a corpus of 180 examples and 59963 words\n",
      "2018-11-06 11:38:30,959 : INFO : Loading a fresh vocabulary\n",
      "2018-11-06 11:38:30,965 : INFO : effective_min_count=5 retains 1884 unique words (33% of original 5569, drops 3685)\n",
      "2018-11-06 11:38:30,966 : INFO : effective_min_count=5 leaves 53740 word corpus (89% of original 59963, drops 6223)\n",
      "2018-11-06 11:38:30,972 : INFO : deleting the raw counts dictionary of 5569 items\n",
      "2018-11-06 11:38:30,973 : INFO : sample=0.001 downsamples 51 most-common words\n",
      "2018-11-06 11:38:30,974 : INFO : downsampling leaves estimated 47285 word corpus (88.0% of prior 53740)\n",
      "2018-11-06 11:38:30,984 : INFO : estimated required memory for 1884 words and 300 dimensions: 5715600 bytes\n",
      "2018-11-06 11:38:30,985 : INFO : resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "model.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train our model. It should take ~10 seconds to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-06 11:38:34,462 : INFO : training model with 3 workers on 1884 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-11-06 11:38:34,549 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:34,554 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:34,562 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:34,564 : INFO : EPOCH - 1 : training on 59963 raw words (47511 effective words) took 0.1s, 495580 effective words/s\n",
      "2018-11-06 11:38:34,651 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:34,652 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:34,656 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:34,657 : INFO : EPOCH - 2 : training on 59963 raw words (47449 effective words) took 0.1s, 556451 effective words/s\n",
      "2018-11-06 11:38:34,724 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:34,725 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:34,727 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:34,728 : INFO : EPOCH - 3 : training on 59963 raw words (47539 effective words) took 0.1s, 694059 effective words/s\n",
      "2018-11-06 11:38:34,802 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:34,810 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:34,810 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:34,811 : INFO : EPOCH - 4 : training on 59963 raw words (47473 effective words) took 0.1s, 602598 effective words/s\n",
      "2018-11-06 11:38:34,878 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:34,879 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:34,880 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:34,882 : INFO : EPOCH - 5 : training on 59963 raw words (47590 effective words) took 0.1s, 695635 effective words/s\n",
      "2018-11-06 11:38:34,949 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:34,950 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:34,952 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:34,952 : INFO : EPOCH - 6 : training on 59963 raw words (47478 effective words) took 0.1s, 715730 effective words/s\n",
      "2018-11-06 11:38:35,014 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:35,016 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:35,021 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:35,022 : INFO : EPOCH - 7 : training on 59963 raw words (47468 effective words) took 0.1s, 724158 effective words/s\n",
      "2018-11-06 11:38:35,084 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:35,087 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:35,088 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:35,089 : INFO : EPOCH - 8 : training on 59963 raw words (47431 effective words) took 0.1s, 732623 effective words/s\n",
      "2018-11-06 11:38:35,158 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:35,160 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:35,161 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:35,161 : INFO : EPOCH - 9 : training on 59963 raw words (47449 effective words) took 0.1s, 679039 effective words/s\n",
      "2018-11-06 11:38:35,226 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:35,227 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:35,227 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:35,228 : INFO : EPOCH - 10 : training on 59963 raw words (47541 effective words) took 0.1s, 735314 effective words/s\n",
      "2018-11-06 11:38:35,293 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:35,295 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:35,296 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:35,297 : INFO : EPOCH - 11 : training on 59963 raw words (47543 effective words) took 0.1s, 715171 effective words/s\n",
      "2018-11-06 11:38:35,357 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:35,359 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:35,362 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:35,362 : INFO : EPOCH - 12 : training on 59963 raw words (47544 effective words) took 0.1s, 758771 effective words/s\n",
      "2018-11-06 11:38:35,424 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:35,425 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:35,426 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:35,427 : INFO : EPOCH - 13 : training on 59963 raw words (47492 effective words) took 0.1s, 765285 effective words/s\n",
      "2018-11-06 11:38:35,488 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:35,491 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:35,492 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:35,492 : INFO : EPOCH - 14 : training on 59963 raw words (47447 effective words) took 0.1s, 750545 effective words/s\n",
      "2018-11-06 11:38:35,568 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:35,570 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:35,572 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:35,572 : INFO : EPOCH - 15 : training on 59963 raw words (47392 effective words) took 0.1s, 607104 effective words/s\n",
      "2018-11-06 11:38:35,647 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:35,648 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:35,652 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:35,652 : INFO : EPOCH - 16 : training on 59963 raw words (47495 effective words) took 0.1s, 621929 effective words/s\n",
      "2018-11-06 11:38:35,718 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:35,718 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:35,719 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:35,719 : INFO : EPOCH - 17 : training on 59963 raw words (47412 effective words) took 0.1s, 726069 effective words/s\n",
      "2018-11-06 11:38:35,791 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:35,796 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:35,798 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:35,799 : INFO : EPOCH - 18 : training on 59963 raw words (47464 effective words) took 0.1s, 608363 effective words/s\n",
      "2018-11-06 11:38:35,861 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:35,863 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:35,864 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:35,865 : INFO : EPOCH - 19 : training on 59963 raw words (47381 effective words) took 0.1s, 748630 effective words/s\n",
      "2018-11-06 11:38:35,927 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:35,929 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:35,930 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:35,931 : INFO : EPOCH - 20 : training on 59963 raw words (47404 effective words) took 0.1s, 739311 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-06 11:38:35,996 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:35,997 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:35,997 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:35,998 : INFO : EPOCH - 21 : training on 59963 raw words (47401 effective words) took 0.1s, 736468 effective words/s\n",
      "2018-11-06 11:38:36,064 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:36,066 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:36,067 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:36,068 : INFO : EPOCH - 22 : training on 59963 raw words (47510 effective words) took 0.1s, 703405 effective words/s\n",
      "2018-11-06 11:38:36,136 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:36,137 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:36,139 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:36,140 : INFO : EPOCH - 23 : training on 59963 raw words (47433 effective words) took 0.1s, 682179 effective words/s\n",
      "2018-11-06 11:38:36,210 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:36,211 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:36,212 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:36,212 : INFO : EPOCH - 24 : training on 59963 raw words (47445 effective words) took 0.1s, 707651 effective words/s\n",
      "2018-11-06 11:38:36,275 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:36,279 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:36,279 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:36,280 : INFO : EPOCH - 25 : training on 59963 raw words (47530 effective words) took 0.1s, 735536 effective words/s\n",
      "2018-11-06 11:38:36,336 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:36,337 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:36,339 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:36,339 : INFO : EPOCH - 26 : training on 59963 raw words (47479 effective words) took 0.1s, 822981 effective words/s\n",
      "2018-11-06 11:38:36,401 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:36,403 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:36,403 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:36,404 : INFO : EPOCH - 27 : training on 59963 raw words (47397 effective words) took 0.1s, 764280 effective words/s\n",
      "2018-11-06 11:38:36,463 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:36,466 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:36,468 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:36,469 : INFO : EPOCH - 28 : training on 59963 raw words (47461 effective words) took 0.1s, 755549 effective words/s\n",
      "2018-11-06 11:38:36,529 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:36,531 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:36,532 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:36,533 : INFO : EPOCH - 29 : training on 59963 raw words (47527 effective words) took 0.1s, 770267 effective words/s\n",
      "2018-11-06 11:38:36,589 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:36,591 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:36,591 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:36,592 : INFO : EPOCH - 30 : training on 59963 raw words (47542 effective words) took 0.1s, 840845 effective words/s\n",
      "2018-11-06 11:38:36,654 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:36,655 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:36,657 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:36,658 : INFO : EPOCH - 31 : training on 59963 raw words (47425 effective words) took 0.1s, 746512 effective words/s\n",
      "2018-11-06 11:38:36,713 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:36,715 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:36,716 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:36,717 : INFO : EPOCH - 32 : training on 59963 raw words (47490 effective words) took 0.1s, 826128 effective words/s\n",
      "2018-11-06 11:38:36,773 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:36,774 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:36,776 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:36,777 : INFO : EPOCH - 33 : training on 59963 raw words (47392 effective words) took 0.1s, 823411 effective words/s\n",
      "2018-11-06 11:38:36,838 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:36,839 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:36,842 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:36,842 : INFO : EPOCH - 34 : training on 59963 raw words (47480 effective words) took 0.1s, 750208 effective words/s\n",
      "2018-11-06 11:38:36,901 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:36,902 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:36,903 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:36,903 : INFO : EPOCH - 35 : training on 59963 raw words (47467 effective words) took 0.1s, 811353 effective words/s\n",
      "2018-11-06 11:38:36,965 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:36,967 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:36,969 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:36,970 : INFO : EPOCH - 36 : training on 59963 raw words (47445 effective words) took 0.1s, 741264 effective words/s\n",
      "2018-11-06 11:38:37,031 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:37,032 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:37,033 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:37,034 : INFO : EPOCH - 37 : training on 59963 raw words (47382 effective words) took 0.1s, 758480 effective words/s\n",
      "2018-11-06 11:38:37,102 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:37,102 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:37,104 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:37,105 : INFO : EPOCH - 38 : training on 59963 raw words (47444 effective words) took 0.1s, 696864 effective words/s\n",
      "2018-11-06 11:38:37,169 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:37,170 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:37,171 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:37,171 : INFO : EPOCH - 39 : training on 59963 raw words (47498 effective words) took 0.1s, 746008 effective words/s\n",
      "2018-11-06 11:38:37,235 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:37,236 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:37,238 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:37,239 : INFO : EPOCH - 40 : training on 59963 raw words (47521 effective words) took 0.1s, 728406 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-06 11:38:37,303 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:37,305 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:37,309 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:37,309 : INFO : EPOCH - 41 : training on 59963 raw words (47412 effective words) took 0.1s, 693291 effective words/s\n",
      "2018-11-06 11:38:37,370 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:37,371 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:37,373 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:37,374 : INFO : EPOCH - 42 : training on 59963 raw words (47518 effective words) took 0.1s, 764907 effective words/s\n",
      "2018-11-06 11:38:37,451 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:37,452 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:37,453 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:37,454 : INFO : EPOCH - 43 : training on 59963 raw words (47377 effective words) took 0.1s, 618039 effective words/s\n",
      "2018-11-06 11:38:37,535 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:37,536 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:37,536 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:37,537 : INFO : EPOCH - 44 : training on 59963 raw words (47533 effective words) took 0.1s, 595187 effective words/s\n",
      "2018-11-06 11:38:37,619 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:37,623 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:37,627 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:37,629 : INFO : EPOCH - 45 : training on 59963 raw words (47495 effective words) took 0.1s, 558101 effective words/s\n",
      "2018-11-06 11:38:37,704 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:37,706 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:37,707 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:37,708 : INFO : EPOCH - 46 : training on 59963 raw words (47493 effective words) took 0.1s, 630313 effective words/s\n",
      "2018-11-06 11:38:37,781 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:37,784 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:37,788 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:37,789 : INFO : EPOCH - 47 : training on 59963 raw words (47370 effective words) took 0.1s, 612101 effective words/s\n",
      "2018-11-06 11:38:37,859 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:37,860 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:37,860 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:37,861 : INFO : EPOCH - 48 : training on 59963 raw words (47501 effective words) took 0.1s, 696084 effective words/s\n",
      "2018-11-06 11:38:37,932 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:37,933 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:37,935 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:37,936 : INFO : EPOCH - 49 : training on 59963 raw words (47332 effective words) took 0.1s, 667564 effective words/s\n",
      "2018-11-06 11:38:37,993 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:37,994 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:37,996 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:37,996 : INFO : EPOCH - 50 : training on 59963 raw words (47555 effective words) took 0.1s, 819125 effective words/s\n",
      "2018-11-06 11:38:38,056 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:38,058 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:38,060 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:38,060 : INFO : EPOCH - 51 : training on 59963 raw words (47321 effective words) took 0.1s, 761798 effective words/s\n",
      "2018-11-06 11:38:38,119 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:38,119 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:38,122 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:38,123 : INFO : EPOCH - 52 : training on 59963 raw words (47386 effective words) took 0.1s, 799107 effective words/s\n",
      "2018-11-06 11:38:38,183 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:38,184 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:38,185 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:38,185 : INFO : EPOCH - 53 : training on 59963 raw words (47451 effective words) took 0.1s, 792244 effective words/s\n",
      "2018-11-06 11:38:38,243 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:38,243 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:38,244 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:38,244 : INFO : EPOCH - 54 : training on 59963 raw words (47514 effective words) took 0.1s, 841120 effective words/s\n",
      "2018-11-06 11:38:38,305 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:38,307 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:38,309 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:38,310 : INFO : EPOCH - 55 : training on 59963 raw words (47459 effective words) took 0.1s, 744819 effective words/s\n",
      "2018-11-06 11:38:38,366 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:38,367 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:38,369 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:38,369 : INFO : EPOCH - 56 : training on 59963 raw words (47476 effective words) took 0.1s, 829291 effective words/s\n",
      "2018-11-06 11:38:38,428 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:38,433 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:38,436 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:38,437 : INFO : EPOCH - 57 : training on 59963 raw words (47492 effective words) took 0.1s, 730119 effective words/s\n",
      "2018-11-06 11:38:38,499 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:38,501 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:38,502 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:38,502 : INFO : EPOCH - 58 : training on 59963 raw words (47465 effective words) took 0.1s, 748637 effective words/s\n",
      "2018-11-06 11:38:38,565 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:38,567 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:38,568 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:38,569 : INFO : EPOCH - 59 : training on 59963 raw words (47433 effective words) took 0.1s, 737353 effective words/s\n",
      "2018-11-06 11:38:38,633 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:38,634 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:38,635 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:38,635 : INFO : EPOCH - 60 : training on 59963 raw words (47441 effective words) took 0.1s, 732558 effective words/s\n",
      "2018-11-06 11:38:38,698 : INFO : worker thread finished; awaiting finish of 2 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-06 11:38:38,700 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:38,701 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:38,702 : INFO : EPOCH - 61 : training on 59963 raw words (47398 effective words) took 0.1s, 731757 effective words/s\n",
      "2018-11-06 11:38:38,764 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:38,765 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:38,766 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:38,766 : INFO : EPOCH - 62 : training on 59963 raw words (47413 effective words) took 0.1s, 762978 effective words/s\n",
      "2018-11-06 11:38:38,823 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:38,825 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:38,828 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:38,829 : INFO : EPOCH - 63 : training on 59963 raw words (47346 effective words) took 0.1s, 794982 effective words/s\n",
      "2018-11-06 11:38:38,882 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:38,884 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:38,884 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:38,885 : INFO : EPOCH - 64 : training on 59963 raw words (47554 effective words) took 0.1s, 879536 effective words/s\n",
      "2018-11-06 11:38:38,940 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:38,941 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:38,943 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:38,944 : INFO : EPOCH - 65 : training on 59963 raw words (47465 effective words) took 0.1s, 833894 effective words/s\n",
      "2018-11-06 11:38:39,004 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:39,005 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:39,005 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:39,006 : INFO : EPOCH - 66 : training on 59963 raw words (47582 effective words) took 0.1s, 805930 effective words/s\n",
      "2018-11-06 11:38:39,065 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:39,067 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:39,068 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:39,069 : INFO : EPOCH - 67 : training on 59963 raw words (47489 effective words) took 0.1s, 785666 effective words/s\n",
      "2018-11-06 11:38:39,124 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:39,125 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:39,126 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:39,126 : INFO : EPOCH - 68 : training on 59963 raw words (47439 effective words) took 0.1s, 861310 effective words/s\n",
      "2018-11-06 11:38:39,187 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:39,188 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:39,189 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:39,189 : INFO : EPOCH - 69 : training on 59963 raw words (47493 effective words) took 0.1s, 778151 effective words/s\n",
      "2018-11-06 11:38:39,248 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:39,251 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:39,253 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:39,254 : INFO : EPOCH - 70 : training on 59963 raw words (47474 effective words) took 0.1s, 766313 effective words/s\n",
      "2018-11-06 11:38:39,308 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:39,310 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:39,313 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:39,314 : INFO : EPOCH - 71 : training on 59963 raw words (47568 effective words) took 0.1s, 815939 effective words/s\n",
      "2018-11-06 11:38:39,372 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:39,373 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:39,376 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:39,377 : INFO : EPOCH - 72 : training on 59963 raw words (47441 effective words) took 0.1s, 778496 effective words/s\n",
      "2018-11-06 11:38:39,437 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:39,438 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:39,438 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:39,439 : INFO : EPOCH - 73 : training on 59963 raw words (47500 effective words) took 0.1s, 788136 effective words/s\n",
      "2018-11-06 11:38:39,498 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:39,501 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:39,503 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:39,504 : INFO : EPOCH - 74 : training on 59963 raw words (47509 effective words) took 0.1s, 759708 effective words/s\n",
      "2018-11-06 11:38:39,557 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:39,558 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:39,561 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:39,561 : INFO : EPOCH - 75 : training on 59963 raw words (47433 effective words) took 0.1s, 851764 effective words/s\n",
      "2018-11-06 11:38:39,620 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:39,621 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:39,622 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:39,623 : INFO : EPOCH - 76 : training on 59963 raw words (47461 effective words) took 0.1s, 795115 effective words/s\n",
      "2018-11-06 11:38:39,683 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:39,685 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:39,686 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:39,687 : INFO : EPOCH - 77 : training on 59963 raw words (47477 effective words) took 0.1s, 782408 effective words/s\n",
      "2018-11-06 11:38:39,747 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:39,748 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:39,750 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:39,750 : INFO : EPOCH - 78 : training on 59963 raw words (47449 effective words) took 0.1s, 770795 effective words/s\n",
      "2018-11-06 11:38:39,806 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:39,807 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:39,807 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:39,808 : INFO : EPOCH - 79 : training on 59963 raw words (47482 effective words) took 0.1s, 858246 effective words/s\n",
      "2018-11-06 11:38:39,867 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:39,869 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:39,870 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:39,870 : INFO : EPOCH - 80 : training on 59963 raw words (47462 effective words) took 0.1s, 798796 effective words/s\n",
      "2018-11-06 11:38:39,932 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:39,933 : INFO : worker thread finished; awaiting finish of 1 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-06 11:38:39,934 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:39,934 : INFO : EPOCH - 81 : training on 59963 raw words (47533 effective words) took 0.1s, 767864 effective words/s\n",
      "2018-11-06 11:38:39,996 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:39,996 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:39,997 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:39,998 : INFO : EPOCH - 82 : training on 59963 raw words (47554 effective words) took 0.1s, 785563 effective words/s\n",
      "2018-11-06 11:38:40,054 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:40,059 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:40,061 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:40,062 : INFO : EPOCH - 83 : training on 59963 raw words (47536 effective words) took 0.1s, 766376 effective words/s\n",
      "2018-11-06 11:38:40,115 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:40,116 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:40,117 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:40,118 : INFO : EPOCH - 84 : training on 59963 raw words (47385 effective words) took 0.1s, 875776 effective words/s\n",
      "2018-11-06 11:38:40,172 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:40,173 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:40,174 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:40,174 : INFO : EPOCH - 85 : training on 59963 raw words (47495 effective words) took 0.1s, 873436 effective words/s\n",
      "2018-11-06 11:38:40,233 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:40,235 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:40,237 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:40,237 : INFO : EPOCH - 86 : training on 59963 raw words (47441 effective words) took 0.1s, 780796 effective words/s\n",
      "2018-11-06 11:38:40,296 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:40,297 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:40,299 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:40,299 : INFO : EPOCH - 87 : training on 59963 raw words (47513 effective words) took 0.1s, 791184 effective words/s\n",
      "2018-11-06 11:38:40,353 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:40,354 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:40,354 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:40,355 : INFO : EPOCH - 88 : training on 59963 raw words (47439 effective words) took 0.1s, 887693 effective words/s\n",
      "2018-11-06 11:38:40,415 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:40,416 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:40,416 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:40,417 : INFO : EPOCH - 89 : training on 59963 raw words (47531 effective words) took 0.1s, 797931 effective words/s\n",
      "2018-11-06 11:38:40,472 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:40,473 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:40,475 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:40,475 : INFO : EPOCH - 90 : training on 59963 raw words (47463 effective words) took 0.1s, 838845 effective words/s\n",
      "2018-11-06 11:38:40,537 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:40,538 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:40,538 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:40,539 : INFO : EPOCH - 91 : training on 59963 raw words (47411 effective words) took 0.1s, 778453 effective words/s\n",
      "2018-11-06 11:38:40,600 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:40,601 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:40,602 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:40,602 : INFO : EPOCH - 92 : training on 59963 raw words (47453 effective words) took 0.1s, 775358 effective words/s\n",
      "2018-11-06 11:38:40,662 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:40,663 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:40,664 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:40,665 : INFO : EPOCH - 93 : training on 59963 raw words (47475 effective words) took 0.1s, 793414 effective words/s\n",
      "2018-11-06 11:38:40,725 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:40,727 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:40,728 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:40,729 : INFO : EPOCH - 94 : training on 59963 raw words (47428 effective words) took 0.1s, 765980 effective words/s\n",
      "2018-11-06 11:38:40,784 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:40,785 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:40,786 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:40,786 : INFO : EPOCH - 95 : training on 59963 raw words (47460 effective words) took 0.1s, 862934 effective words/s\n",
      "2018-11-06 11:38:40,847 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:40,848 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:40,849 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:40,849 : INFO : EPOCH - 96 : training on 59963 raw words (47433 effective words) took 0.1s, 783468 effective words/s\n",
      "2018-11-06 11:38:40,911 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:40,912 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:40,912 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:40,913 : INFO : EPOCH - 97 : training on 59963 raw words (47516 effective words) took 0.1s, 774993 effective words/s\n",
      "2018-11-06 11:38:40,966 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:40,970 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:40,971 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:40,971 : INFO : EPOCH - 98 : training on 59963 raw words (47391 effective words) took 0.1s, 838230 effective words/s\n",
      "2018-11-06 11:38:41,030 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:41,032 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:41,033 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:41,033 : INFO : EPOCH - 99 : training on 59963 raw words (47407 effective words) took 0.1s, 790980 effective words/s\n",
      "2018-11-06 11:38:41,094 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-06 11:38:41,095 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-06 11:38:41,096 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-06 11:38:41,096 : INFO : EPOCH - 100 : training on 59963 raw words (47441 effective words) took 0.1s, 785709 effective words/s\n",
      "2018-11-06 11:38:41,097 : INFO : training on a 5996300 raw words (4746536 effective words) took 6.6s, 715559 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.4 s, sys: 497 ms, total: 16.9 s\n",
      "Wall time: 6.64 s\n"
     ]
    }
   ],
   "source": [
    "%time model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-06 11:38:42,951 : INFO : saving Doc2Vec object under /Users/carlossouza/Dropbox/2018/OMSCS/CS-6460-Education-Technology/bettertogether/models/doc2vec_model_assignment_1, separately None\n",
      "2018-11-06 11:38:43,007 : INFO : saved /Users/carlossouza/Dropbox/2018/OMSCS/CS-6460-Education-Technology/bettertogether/models/doc2vec_model_assignment_1\n"
     ]
    }
   ],
   "source": [
    "model_path = os.path.join(os.getcwd(), \"models\")\n",
    "doc2vec_model_filepath = os.path.join(model_path, 'doc2vec_model_' + assignment_code)\n",
    "if save_files:\n",
    "    model.save(doc2vec_model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Test the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! Now, let's test the result by first looking who are the most similar students of **Carlos Souza**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-06 11:38:46,811 : INFO : precomputing L2-norms of doc weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar documents of Carlos Souza for Assignment 1\n",
      "0.38691163063049316 \tAndrew Wolfe\n",
      "0.3522692918777466 \tNavnit Belur\n",
      "0.34605270624160767 \tImad Rahman\n",
      "0.33395418524742126 \tRichard Doan\n",
      "0.3208683729171753 \tHuan Chen\n",
      "0.3152480125427246 \tLangston Chandler\n",
      "0.30462008714675903 \tShelby Allen\n",
      "0.30134904384613037 \tMark Wahnish\n",
      "0.2821483612060547 \tBraden Whited\n",
      "0.2779601514339447 \tDaniel Fecher\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "test_student = \"Carlos Souza\"\n",
    "\n",
    "similar_doc = model.docvecs.most_similar(test_student) \n",
    "print(\"Most similar documents of \" + test_student + \" for \" + assignment)\n",
    "for item in similar_doc:\n",
    "    print(str(item[1])  + \" \\t\" + str(item[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model says that, based on **Assignment 2**, **Carlos Souza**'s most similar student is **Alaa Shafaee**. Let's see if that's true by retrieving main topics from Carlos Souza's Assignment 2 and Alaa Shafaee's Assignment 2 and compare them. We will do it by using an NPL API called TextRazor.\n",
    "\n",
    "First, let's define a function `print_top_entities` to retrieve main topics of an assignment, and initialize TextRazor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_entities(student, limit=15):\n",
    "    text = textract.process(os.path.join(path, student + \" - \" + assignment + \".pdf\")).decode(\"utf-8\")\n",
    "    response = client.analyze(text)\n",
    "    entities = list(response.entities())\n",
    "    entities.sort(key=lambda x: x.relevance_score, reverse=True)\n",
    "    seen = set()\n",
    "    index = 0\n",
    "    for entity in entities:\n",
    "        if entity.id not in seen:\n",
    "            print(entity.id, entity.relevance_score, entity.confidence_score)\n",
    "            seen.add(entity.id)\n",
    "            index += 1\n",
    "            if index >= limit:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textrazor\n",
    "\n",
    "textrazor.api_key = \"d26228c7df9e0ea2a9c2656b520eefe049ae8c837fb1ef5a95619e04\"\n",
    "client = textrazor.TextRazor(extractors=[\"entities\", \"topics\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's compare Carlos Souza's main topics and Alaa Shafaee's main topics in Assignment 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial intelligence 0.7157 5.663\n",
      "Educational technology 0.6323 2.001\n",
      "Learning theory (education) 0.6189 2.685\n",
      "Learning 0.5775 7.618\n",
      "Web application 0.5759 4.967\n",
      "Gender 0.5633 4.598\n",
      "Internet forum 0.5609 2.506\n",
      "Prototype 0.5608 3.904\n",
      "Mentorship 0.5398 1.392\n",
      "Research 0.5227 7.77\n",
      "Programming language 0.5191 15.69\n",
      "Conditional (computer programming) 0.4961 3.225\n",
      "Computer programming 0.4799 9.991\n",
      "Computer 0.4741 15.63\n",
      "Computer science 0.4592 10.05\n"
     ]
    }
   ],
   "source": [
    "print_top_entities(\"Carlos Souza\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Educational technology 0.8022 3.868\n",
      "Research 0.6652 6.994\n",
      "Watson (computer) 0.654 7.869\n",
      "Teaching method 0.6258 1.319\n",
      "Software 0.5865 10.98\n",
      "Information technology 0.5729 3.136\n",
      "Computer science 0.5696 17.79\n",
      "Artificial intelligence 0.5579 16.76\n",
      "Statistics 0.5565 11.62\n",
      "IBM 0.5542 18.71\n",
      "Intelligence 0.5393 5.171\n",
      "Science 0.5307 13.74\n",
      "Georgia Institute of Technology 0.5217 5.195\n",
      "Library 0.5169 3.375\n",
      "Education 0.4798 1.484\n"
     ]
    }
   ],
   "source": [
    "print_top_entities(\"Andrew Wolfe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, our algorithm worked perfectly! The API identifies all main topics and most of them are the same!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next steps\n",
    "\n",
    "- If you need support, drop me a note at [souza@gatech.edu](mailto:souza@gatech.edu), I will be glad to help.\n",
    "\n",
    "### Further readings\n",
    "\n",
    "- [Distributed Representations of Sentences and Documents](https://cs.stanford.edu/~quocle/paragraph_vector.pdf)\n",
    "- [Doc2Vec Tutorial on the Lee Dataset](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb)\n",
    "- [Modern NLP in Python](https://github.com/skipgram/modern-nlp-in-python/blob/master/executable/Modern_NLP_in_Python.ipynb)\n",
    "- [Gensim - Topic Modelling for Humans - tutorials](https://radimrehurek.com/gensim/tutorial.html)\n",
    "- [TextRazor API tutorials](https://www.textrazor.com/tutorials)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
