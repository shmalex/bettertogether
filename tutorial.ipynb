{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial\n",
    "\n",
    "This is actually less of a tutorial and more the IPython notebook I used to learn how to pre-process and train the models used in the app. It's work in progress, as I intend to comment & explain each step.\n",
    "\n",
    "Here are some important links I found very useful in developing this notebook, and in creating this project:\n",
    "- https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb\n",
    "- https://github.com/skipgram/modern-nlp-in-python/blob/master/executable/Modern_NLP_in_Python.ipynb\n",
    "- https://radimrehurek.com/gensim/tut1.html\n",
    "- https://radimrehurek.com/gensim/tut2.html\n",
    "- https://radimrehurek.com/gensim/tut3.html\n",
    "- https://www.textrazor.com/tutorials#tagging\n",
    "- https://canvasapi.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging, gensim, os, textract\n",
    "import itertools as it\n",
    "from gensim.parsing.preprocessing import preprocess_string, remove_stopwords\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Save all sentences from a given assignment (all students) into a file\n",
    "### Start with Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "assignment = \"Assignment 5\"\n",
    "save_files = True\n",
    "\n",
    "assignment_code = assignment.lower().replace(' ', '_')\n",
    "path = os.path.join(os.getcwd(), \"pdfs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_sentences_filepath = os.path.join(path, 'unigram_sentences_all_' + assignment_code + '.txt')\n",
    "\n",
    "if save_files:\n",
    "    with open(unigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "        for file in os.listdir(path):\n",
    "            if assignment in str(file):\n",
    "                text = textract.process(os.path.join(path, file)).decode(\"utf-8\")\n",
    "                unigram_sentences = []\n",
    "                for sent in gensim.summarization.textcleaner.get_sentences(text):\n",
    "                    processed = preprocess_string(sent)\n",
    "                    if len(processed) > 0:\n",
    "                        f.write(\" \".join(processed) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if it worked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_sentences = gensim.models.word2vec.LineSentence(unigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cut deal cop turn blind ey\n",
      "\n",
      "problem\n",
      "\n",
      "individu dirti cop deepli entrench cultur\n",
      "\n",
      "depart\n",
      "\n",
      "violenc ensu disput meet strong drug\n",
      "\n",
      "demand larg bribe allow violenc continu\n",
      "\n",
      "kumar vimal\n",
      "\n",
      "skaperda stergio\n",
      "\n",
      "econom organ crime\n",
      "\n",
      "crimin law econom februari\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for unigram_sentence in it.islice(unigram_sentences, 230, 240):\n",
    "    print(u' '.join(unigram_sentence))\n",
    "    print(u'')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Use the Unigrams to create Bigrams model, and save all sentences to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-06 17:50:46,355 : INFO : collecting all words and their counts\n",
      "2018-10-06 17:50:46,358 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2018-10-06 17:50:46,495 : INFO : PROGRESS: at sentence #10000, processed 45341 words and 31928 word types\n",
      "2018-10-06 17:50:46,595 : INFO : PROGRESS: at sentence #20000, processed 88508 words and 56809 word types\n",
      "2018-10-06 17:50:46,692 : INFO : PROGRESS: at sentence #30000, processed 132793 words and 79786 word types\n",
      "2018-10-06 17:50:46,795 : INFO : PROGRESS: at sentence #40000, processed 177136 words and 101457 word types\n",
      "2018-10-06 17:50:46,894 : INFO : PROGRESS: at sentence #50000, processed 219625 words and 121591 word types\n",
      "2018-10-06 17:50:46,996 : INFO : PROGRESS: at sentence #60000, processed 262268 words and 141521 word types\n",
      "2018-10-06 17:50:47,084 : INFO : PROGRESS: at sentence #70000, processed 306621 words and 159731 word types\n",
      "2018-10-06 17:50:47,091 : INFO : collected 161262 word types from a corpus of 309968 words (unigram + bigrams) and 70768 sentences\n",
      "2018-10-06 17:50:47,092 : INFO : using 161262 counts as vocab in Phrases<0 vocab, min_count=5, threshold=10.0, max_vocab_size=40000000>\n",
      "2018-10-06 17:50:47,095 : INFO : saving Phrases object under /Users/carlossouza/Dropbox/playground/gensim/pdfs/bigram_model_all_assignment_5, separately None\n",
      "2018-10-06 17:50:47,307 : INFO : saved /Users/carlossouza/Dropbox/playground/gensim/pdfs/bigram_model_all_assignment_5\n"
     ]
    }
   ],
   "source": [
    "bigram_model_filepath = os.path.join(path, 'bigram_model_all_' + assignment_code)\n",
    "bigram_model = gensim.models.Phrases(unigram_sentences)\n",
    "if save_files:\n",
    "    bigram_model.save(bigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "bigram_sentences_filepath = os.path.join(path, 'bigram_sentences_all_' + assignment_code + '.txt')\n",
    "\n",
    "if save_files:\n",
    "    with open(bigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "        for unigram_sentence in unigram_sentences:\n",
    "            bigram_sentence = \" \".join(bigram_model[unigram_sentence])\n",
    "            f.write(bigram_sentence + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if it worked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paper discuss piano plai practic help benefit senior_citizen\n",
      "\n",
      "benefit\n",
      "\n",
      "outlin overal sens physic mental includ lessen stress\n",
      "\n",
      "pain medic usag slow ag relat cognit declin feel pleasur\n",
      "\n",
      "enjoy pride sens accomplish learn new skill creation\n",
      "\n",
      "mainten social connect mean creativ self express construct\n",
      "\n",
      "ident time life sens ident flux\n",
      "\n",
      "“educ gamif\n",
      "\n",
      "game_base learn compar study”\n",
      "\n",
      "paper discuss differ game element motiv user long continu game\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bigram_sentences = gensim.models.word2vec.LineSentence(bigram_sentences_filepath)\n",
    "\n",
    "for bigram_sentence in it.islice(bigram_sentences, 5000, 5010):\n",
    "    print(u' '.join(bigram_sentence))\n",
    "    print(u'')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Use the Bigrams to create Trigrams model, and save all sentences to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-06 17:50:48,939 : INFO : collecting all words and their counts\n",
      "2018-10-06 17:50:48,941 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2018-10-06 17:50:49,056 : INFO : PROGRESS: at sentence #10000, processed 42578 words and 32354 word types\n",
      "2018-10-06 17:50:49,140 : INFO : PROGRESS: at sentence #20000, processed 82887 words and 57898 word types\n",
      "2018-10-06 17:50:49,223 : INFO : PROGRESS: at sentence #30000, processed 124477 words and 81675 word types\n",
      "2018-10-06 17:50:49,307 : INFO : PROGRESS: at sentence #40000, processed 166229 words and 104210 word types\n",
      "2018-10-06 17:50:49,389 : INFO : PROGRESS: at sentence #50000, processed 206262 words and 125109 word types\n",
      "2018-10-06 17:50:49,470 : INFO : PROGRESS: at sentence #60000, processed 246579 words and 145792 word types\n",
      "2018-10-06 17:50:49,556 : INFO : PROGRESS: at sentence #70000, processed 287723 words and 165004 word types\n",
      "2018-10-06 17:50:49,562 : INFO : collected 166604 word types from a corpus of 290857 words (unigram + bigrams) and 70768 sentences\n",
      "2018-10-06 17:50:49,563 : INFO : using 166604 counts as vocab in Phrases<0 vocab, min_count=5, threshold=10.0, max_vocab_size=40000000>\n",
      "2018-10-06 17:50:49,565 : INFO : saving Phrases object under /Users/carlossouza/Dropbox/playground/gensim/pdfs/trigram_model_all_assignment_5, separately None\n",
      "2018-10-06 17:50:49,802 : INFO : saved /Users/carlossouza/Dropbox/playground/gensim/pdfs/trigram_model_all_assignment_5\n"
     ]
    }
   ],
   "source": [
    "trigram_model_filepath = os.path.join(path, 'trigram_model_all_' + assignment_code)\n",
    "trigram_model = gensim.models.Phrases(bigram_sentences)\n",
    "if save_files:\n",
    "    trigram_model.save(trigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "trigram_sentences_filepath = os.path.join(path, 'trigram_sentences_all_' + assignment_code + '.txt')\n",
    "\n",
    "if save_files:\n",
    "    with open(trigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "        for bigram_sentence in bigram_sentences:\n",
    "            trigram_sentence = \" \".join(trigram_model[bigram_sentence])\n",
    "            f.write(trigram_sentence + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if it worked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paper discuss piano plai practic help benefit senior_citizen\n",
      "\n",
      "benefit\n",
      "\n",
      "outlin overal sens physic mental includ lessen stress\n",
      "\n",
      "pain medic usag slow ag relat cognit declin feel pleasur\n",
      "\n",
      "enjoy pride sens accomplish learn new skill creation\n",
      "\n",
      "mainten social connect mean creativ self express construct\n",
      "\n",
      "ident time life sens ident flux\n",
      "\n",
      "“educ gamif\n",
      "\n",
      "game_base_learn compar study”\n",
      "\n",
      "paper discuss differ game element motiv user long continu game\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trigram_sentences = gensim.models.word2vec.LineSentence(trigram_sentences_filepath)\n",
    "\n",
    "for trigram_sentence in it.islice(trigram_sentences, 5000, 5010):\n",
    "    print(u' '.join(trigram_sentence))\n",
    "    print(u'')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Use the Trigram model to transform all documents, and save each document in one line of a big file containing all transformed documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "trigram_assignments_filepath = os.path.join(path, 'trigram_transformed_assignments_all_' + assignment_code + '.txt')\n",
    "\n",
    "if save_files:\n",
    "    with open(trigram_assignments_filepath, 'w', encoding='utf_8') as f:\n",
    "        for file in os.listdir(path):\n",
    "            if assignment in str(file):\n",
    "                text = textract.process(os.path.join(path, file)).decode(\"utf-8\")\n",
    "                unigram_assignment = preprocess_string(text)\n",
    "                bigram_assignment = bigram_model[unigram_assignment]\n",
    "                trigram_assignment = trigram_model[bigram_assignment]\n",
    "                f.write(\" \".join(trigram_assignment) + \"\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Checking the work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "\n",
    "file_list = []\n",
    "for file in os.listdir(path):\n",
    "    if assignment in str(file):\n",
    "        file_list.append(file)\n",
    "\n",
    "random_index = randrange(len(file_list))\n",
    "random_filename = file_list[random_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(trigram_assignments_filepath) as f:\n",
    "    trigram_assignments_list = f.readlines()\n",
    "\n",
    "trigram_assignments_list = [x.strip() for x in trigram_assignments_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assignment #5, CS6460 ET, Fall 2018, Chris Hockett (chockett3)\n",
      "\n",
      "Assignment: Collecting Your Sources\n",
      "Introduction\n",
      "For my project, I intend to create a tool which will visualize the process of lexical analysis. If I am\n",
      "exceptionally proficient, I will tackle a tool which will visualize the process of grammatical analysis too\n",
      "(to be clear, this last piece is a stretch goal). A tool set like this stimulates learning for those who most\n",
      "apply their spatial intelligence during study. I happen to be a learner who fits into this category‐ once I\n",
      "can see how something works (i.e. its behavior) I almost immediately understand what I’m studying.\n",
      "The project will require research in multiple areas:\n",
      "1) Lexical Analysis;\n",
      "2) Graphical Visualization;\n",
      "3) Spatial Intelligence and its associated best practice for teaching these learners.\n",
      "With that in mind, here is my annotated bibliography, subdivided into the relevant categories.\n",
      "\n",
      "Lexical Analysis\n",
      "van Engelen, Robert (6/27/2017) Constructing Fast Lexical Analyzers with RE/Flex. Retrieved from\n",
      "Genivia website: https://www.genivia.com/\n",
      "This website outlines a lexer which reflects the tool closest to the one that I imagine for\n",
      "graphing the lexical analysis state machine. However, it does not address the other visualization\n",
      "I have in mind‐ visualizing the actual processing of an input stream.\n",
      "The website gives important information on using the tool and such, but does not drill too\n",
      "deeply into the actual mechanics of lexical analysis nor the inner workings of a lexer. However,\n",
      "the value to my project is the proof, that in fact, at least one part of my ideas can be realized.\n",
      "Brouwer K., Gellerich W., Ploedereder E. (1998) Myths and facts about the efficient implementation\n",
      "of finite automata and lexical analysis. In: Koskimies K. (eds) Compiler Construction. CC 1998.\n",
      "Lecture Notes in Computer Science, Vol 1383. Springer, Berlin, Heidelberg\n",
      "This paper is an interesting read pertaining to measuring the performance of a number of\n",
      "implementations of lexical analysis algorithms. A real world language (Ada) was used as\n",
      "their test case. The authors evaluated both table‐driven and hand‐crafted scanners for a\n",
      "total of 12 algorithms analyzed.\n",
      "One of the results, which I found particularly interesting, was the assertion that the table‐\n",
      "driven analyzers were slower due to having many goto branches (i.e. unstructured code).\n",
      "The authors showed structured code leads to better performance due to optimizations the\n",
      "\n",
      "\f",
      "Assignment #5, CS6460 ET, Fall 2018, Chris Hockett (chockett3)\n",
      "compiler can perform. I was interested in the paper because they listed many lexical\n",
      "scanner generators I was not aware existed.\n",
      "DeRemer F.L. (1976) Lexical analysis. In: Brauer F.L. et al. (eds) Compiler Construction. Lecture\n",
      "Notes in Computer Science, Vol 21. Springer, Berlin, Heidelberg\n",
      "This paper, which really is really a set of chapters in this classic compendium of lecture\n",
      "notes on computer science. It is a really good introduction to the concepts that will be\n",
      "explored throughout my project. I would classify its content as rudimentary through\n",
      "advanced depending on the chapter and the topic. For my purposes, the text dealing with\n",
      "lexical analysis held most importance. I found it to be an easy read and great to refresh my\n",
      "memory on the subtleties of the subject matter.\n",
      "Lesk, M.E., Schmidt, E. (7/21/1975) Lex‐ A Lexical Analyzer Generator. AT&T Bell Laboratories, Murray\n",
      "Hill, New Jersey 07974\n",
      "This paper outlines the tool which created the genre of lexical analyzer generators‐ Lex. The\n",
      "paper is a user manual of sorts, explains what it produces after processing the user’s input as\n",
      "well as how to interface to other compiler generator tools.\n",
      "The paper, however, does not delve into the inner workings of the tables or code fragments it\n",
      "generates. My project revolves around visualizing the mechanics of table driven lexical analyzers\n",
      "which will require me to derive that information elsewhere. The real value of this paper is in its\n",
      "description of the use of this class of tool. Also, from a historical perspective, I find it a neat trip\n",
      "back in time. I plan to focus on the type of analyzer generated by this tool.\n",
      "Heuring, V.P, (9/27/1985) The Automatic Generation of Fast Lexical Analysers. Department of Electrical\n",
      "and Computer Engineering, University of Colorado, Boulder, Colorado, 80309\n",
      "Heuring discusses the “why’s” and “how’s” of automatic lexical analyzers. The discussion\n",
      "revolves around how to improve the execution speed of table‐driven analyzers. The\n",
      "performance discussions in this paper, while interesting on their own, are not the primary\n",
      "appeal. Rather, the discussion of the recognition algorithm(s) are relevant to my project‐ one\n",
      "has to understand how these table‐driven analyzers are constructed if one plans to modify the\n",
      "algorithms to produce supplemental information. In my case, that would be visualization data\n",
      "for the tables (i.e. finite state machines) and internal machinery.\n",
      "Rus, T., Halverson, T (Unknown) A Language Independent Scanner Generator. Department of Computer\n",
      "Science, The University of Iowa, Iowa City, Iowa, 52252\n",
      "Rus and Halverson address an alternate implementation for lexical analysis. The implementation\n",
      "they describe is somewhat orthogonal to the traditional table‐driven implementations. They still\n",
      "\n",
      "\f",
      "Assignment #5, CS6460 ET, Fall 2018, Chris Hockett (chockett3)\n",
      "use finite state machines, but in a two‐level approach to expression recognition. The authors\n",
      "provide a very detailed explanation of their method and various results they attained.\n",
      "The value of this paper, to my project, is the authors describe a type of analyzer I do not intend\n",
      "to explore. If I had more time, I would like to work with this scanner generator algorithm as well,\n",
      "but it does not fit with the schedule.\n",
      "Thompson, K. (6/1968). Regular Expression Search Algorithm. Communications of the Association for\n",
      "Computing Machinery, Vol. 11, Issue 6 419‐422.\n",
      "McNaughton, R., Yamada, H. (3/1960) Regular Expressions and State Graphs for Automata. IRE\n",
      "Transactions on Electronic Computers, Vol. 9 EC‐9 39‐47\n",
      "I place these two items into my bibliography more as potential rather than direct references.\n",
      "The two works are considered to be foundational in the domain of lexical analyzers. They are\n",
      "referenced extensively in the literature even though they are fifty years or older. A regular\n",
      "expression, typically, is the mechanism used to articulate the token for which one is trying to\n",
      "locate a match.\n",
      "I have not formulated a strategy for my final paper, but if I do delve into the topic of regular\n",
      "expressions during any discussion, I’ll likely reference these two papers. If I don’t, I feel there is\n",
      "no harm including them here.\n",
      "\n",
      "Graphical Visualization\n",
      "Graphviz (current) Graph Visualization Software. Retrieved from Graphviz website:\n",
      "https://www.graphviz.org\n",
      "Graphviz is a publicly available open‐source package for drawing graphs. As part of the package,\n",
      "libraries are available to applications. Graphviz is short for Graph Visualization. It uses a\n",
      "description of a graph in a text language and then creates the corresponding graph(s). One can\n",
      "decorate these graphs with fonts, colors, layout and line styles and the like. It has many\n",
      "different layout engines designed for different graph types such as size and style. My primary\n",
      "interest in this package is to render graphs of the various state machines.\n",
      "Tang, S.K. (2014) Simulating Transparency and Cutaway to Visualize 3D Internal Information for Tangible\n",
      "UIs. M.S. Thesis, Massachusetts Institute of Technology, 2014\n",
      "Tang’s thesis centers on the concept of applying Tangible UIs to Computer‐Aided Design (CAD)\n",
      "systems. He proceeds to discuss the reasons why his arguments are correct and proposes a\n",
      "novel technique for better UI interaction with the application’s user. From a casual observer’s\n",
      "perspective (me), the paper is an interesting read. Most of the topic, however, is not relevant to\n",
      "my project.\n",
      "\n",
      "\f",
      "Assignment #5, CS6460 ET, Fall 2018, Chris Hockett (chockett3)\n",
      "The section detailing why cutaway views are important and the numerous examples of various\n",
      "domains using cutaways to enhance a person’s understanding is relevant to my project. What I\n",
      "propose to implement is a cutaway view of the inner workings of a lexical analyzer.\n",
      "Li, W., Ritter, L., Agrawala, M., Curless, B. Salesin, D. (2007) Interactive Cutaway Illustrations of Complex\n",
      "3D Models. Association for Computing Machinery Transactions on Graphics, Vol. 26, No. 3, Article 31,\n",
      "2007\n",
      "The value of this article is the extensive exploration of the topic of cutaway views, their\n",
      "application to understanding and how best (they view) the user should interact with cutaway\n",
      "views. The authors spend their time analyzing two primary domains: mechanical assemblies and\n",
      "anatomical models. They include arguments for the need of cutaway illustrations to support\n",
      "interactive exploration. They assert this helps users understand the spatial relationships\n",
      "between parts. This is the key point to my project‐ I believe, once the user can “see” inside the\n",
      "analysis machinery, they will gain a greater understanding of how the parts work in unison to\n",
      "tokenize the input stream.\n",
      "Ifenthaler, D. (2010) Bridging the Gap between Expert‐Novice Differences: The Model‐Based Feedback\n",
      "Approach. Journal of Research on Technology in Education, Vol. 43, No 2, 2010, 103‐117\n",
      "Ifenthaler devotes the entire paper to making the argument that model‐based feedback is an\n",
      "important component of learning/understanding. Specifically, this paper introduces two forms\n",
      "of model‐based feedback‐ the cutaway model and the discrepancy model. Ifenthaler goes on to\n",
      "provide results of improved comprehension by students based on the newly introduced models.\n",
      "I don’t see the discrepancy model as germane to my project. However, the assertion that\n",
      "cutaway models improve comprehension is relevant. The argument that a cutaway model is a\n",
      "legitimate form of feedback for the learning is important to my position.\n",
      "Lidal, E.M., Hauser, H., Viola, I. (2013) Design Principles for Cutaway Visualization of Geological Models.\n",
      "Association for Computing Machinery, 978‐1‐4503‐1977‐5/0005. SCCG 2012, Budmerice, Slovakia, May\n",
      "204, 2012\n",
      "The authors explore, at depth, the design principles of cutaway visualizations. Admittedly, the\n",
      "earth sciences part of the paper holds no interest to me or relevance to my project, the design\n",
      "principles they propose do apply, however. They outline five principles of a good cutaway\n",
      "model. Not all five will be relevant to my project. For example, illumination is not applicable to\n",
      "this domain. But, others are such as applying viewpoint, keep it simple and include familiar\n",
      "context principles I plan to apply.\n",
      "\n",
      "\f",
      "Assignment #5, CS6460 ET, Fall 2018, Chris Hockett (chockett3)\n",
      "Spatial Intelligence\n",
      "Brualdi, Amy (1998) Gardner’s Theory. Teacher Librarian, Nov/Dec 1998; 26, 2, 26‐28\n",
      "Brualdi reviews Gardner’s Theory of Multiple Intelligences (Gardner, 1983). Of particular interest\n",
      "to my project is that of the Spatial Intelligence. Briefly, that is a person’s ability to create and\n",
      "manipulate mental images to solve problems. The main argument of this paper is that the\n",
      "educational system should not focus on a few of the multiple intelligences everyone posses.\n",
      "Instead, the educational system should take a broader view and teach to the strength of the\n",
      "student’s skills. The same assertion is made for testing. My project aims to stimulate the spatial\n",
      "intelligence of the learner via visual simulation which agrees with the author’s contention that\n",
      "some folks just learn better by visualizing solutions.\n",
      "Cooper, Eileen E. (2000) Spatial‐Temporal Intelligence: Original Thinking Processes of Gifted Inventors.\n",
      "Journal for the Education of the Gifted, Vol. 24, No.2, 2000, 170‐193\n",
      "This work focuses on the argument that spatial intelligence should really be redefined as spatial‐\n",
      "temporal intelligence (adding the time dimension to spatial intelligence). I accept her reasoning\n",
      "and arguments, but didn’t pick this article for that reason. Instead, she very accurately (to me)\n",
      "defined spatial intelligence containing the following skills “… visualizing imagery; perceiving\n",
      "figures as wholes; generating a whole image from a fragment; … understanding spatial\n",
      "relationships from multi‐perspectives and among internal movement of parts …” (p. 171) It’s the\n",
      "last bit of that definition which is the cornerstone of my project‐ some people simply\n",
      "understand problems better (or completely) once they can visualize the internal movement of\n",
      "the parts. In this case, the parts are the internal machinery of a lexical analyzer.\n",
      "Gardner, H. (1995). Reflections on multiple intelligences: Myths and messages. Phi Delta Kappan, 77,\n",
      "200‐209\n",
      "The creator of the Theory of Multiple Intelligences discusses the debates that have stemmed\n",
      "from his original theory. Gardner addresses many myths and the sets the record straight, as it\n",
      "were. I’m not about the debate and leave it to him to discuss his theories in detail and defend\n",
      "his assertions. But, what struck me as applicable to my project is that when in an educational\n",
      "setting, it’s not wise to attempt to teach to all seven intelligences. After reading that statement,\n",
      "it dawned on me that the tool I propose it not for everyone (never said it was, but wanted to\n",
      "clarify my position) and is best used by the class of folks who best learn by visualization. And,\n",
      "frankly, I grant the tool may have no practical use to others who learn by different means.\n",
      "ELIOT, J. (2002). About Spatial Intelligence: I. Perceptual and Motor Skills, 2002, 94, 479‐486\n",
      "Eliot describes a multitude of ways to research subjects and their spatial intelligence. The\n",
      "analysis of research styles is of little application to my project. However, what leapt off the\n",
      "paper was his assertion that spatial intelligence contributes significantly to the technical,\n",
      "scientific and mathematical occupations (p. 483). This bolsters my position that a tool like I\n",
      "propose would be useful to a whole class of (computer) scientists rather than being a pet\n",
      "project.\n",
      "\n",
      "\f",
      "Assignment #5, CS6460 ET, Fall 2018, Chris Hockett (chockett3)\n",
      "Newcombe , N, S. Frick, A (2000). Early Education for Spatial Intelligence: Why, What, and How. Mind,\n",
      "Brain, and Education, 2000, Vol. 4‐3, 102‐111\n",
      "The authors lay out the case for integrating spatial content into formal instruction. They\n",
      "articulate a number of applications of spatial intelligence to the population in general. Of\n",
      "particular interest is their discussion of spatial intelligence in the science, technology,\n",
      "engineering and mathematics (STEM) fields. They spend a fair amount of time discussing mental\n",
      "rotation as a measure of mental development in children followed by observations and\n",
      "suggestions for intervention, if necessary. My take‐away from this paper is their implication that\n",
      "students should practice their spatial skills in the classroom. My proposed tool helps do just\n",
      "that.\n",
      "Özdener, N., Özcoban, T (2004). A Project Based Learning Model’s Effectiveness on Computer Courses\n",
      "and Multiple Intelligence Theory. Educational Sciences: Theory & Practice May 2004, 164‐170\n",
      "The authors outline their research into optimal learning techniques for learners with stronger\n",
      "intelligence skills. They broke a group of students into control sets‐ those with the same\n",
      "predominant intelligence and another set with a blend of intelligences. Then, they set to\n",
      "determine if traditional instruction was better, worse or the same as project based instruction.\n",
      "They determined that project based instruction was superior to traditional (explanation based)\n",
      "instruction. Furthermore, they found that groups comprised of students with differing dominant\n",
      "intelligences performed better than a group with members having a single dominant\n",
      "intelligence. This applies to my project in that my tool is designed for the spatial intelligence\n",
      "dominate student to use in a project based learning environment.\n",
      "Gardner, G., Hatch, T. (1989). Multiple Intelligences Go to School. Educational Implications of the Theory\n",
      "of Multiple Intelligences. Educational Researcher, Vol. 18, No. 8. 4‐10\n",
      "Gardner and Hatch embark on a conversation about a new approach to assigning human\n",
      "intelligence. By that, they describe tests and observations of the seven intelligences in school\n",
      "age children. They observe that students as young as 3 exhibit strengths and weaknesses on the\n",
      "intelligence spectrum. Furthermore, social class may play a factor in the development of the\n",
      "various intelligences.\n",
      "I included this paper not for its research on testing methods, but instead, because it seems\n",
      "important to me for students to understand their intelligence strengths and weaknesses. If they\n",
      "know they have a strength in spatial intelligence, they are likely to seek out learning aids (such\n",
      "as my tool) to assist their studies.\n",
      "Hegarty, M. (2010). Components of Spatial Intelligence. Psychology of Learning and Motivation, Vol. 52,\n",
      "265‐297\n",
      "This paper’s value is in the in‐depth definition of spatial intelligence. At its core, spatial\n",
      "intelligence (or spatial thinking in the author’s words) is “the thinking about the shapes and\n",
      "arrangements of objects in space and spatial processes, such as the deformation of objects, and\n",
      "the movement of objects and other entities through space” (page 266). She goes on to say “it\n",
      "can also involve thinking with spatial representations of non‐spatial entities” and gives examples\n",
      "of using org charts to think about the structure of a company or a graph to evaluate changes in\n",
      "\n",
      "\f",
      "Assignment #5, CS6460 ET, Fall 2018, Chris Hockett (chockett3)\n",
      "the cost of healthcare (page 266). She also discusses how spatial thinking is under instructed in\n",
      "the school curriculum. My project supports adding spatial thinking skills in the school\n",
      "curriculum.\n",
      "Prokysek, M., Rambousek, V., Widova, R. (2013). Research into spatial intelligence and the efficiency of\n",
      "the application of spatial visualization in instruction. Procedia‐ Social and Behavioral Sciences 84 (2013)\n",
      "855‐859\n",
      "This paper looks at spatial intelligence and whether or not adding spatial visualization during\n",
      "instruction boosts student’s comprehension (regardless of dominant intelligence). They\n",
      "conclude that it will increase a student’s comprehension and, by extension, their scores on\n",
      "subsequent tests.\n",
      "Also, they discuss a few types of visualization such as 3D, 2D, planer, spatial, etc. The authors do\n",
      "not tackle the subject of which type of visualization works best in which situation, however.\n",
      "They mention they believe that spatial visualizations will become more main stream in\n",
      "educational settings and their belief is that such visualization aids will contribute to higher\n",
      "quality education. I’ve been looking at my project as a vehicle for dominate spatial intelligence\n",
      "learners, but they make the case for the benefit of any intelligence type using spatial\n",
      "visualizations.\n",
      "Lubinski, D. (2010). Spatial ability and STEM: A sleeping giant for talent identification and development.\n",
      "Personality and Individual Differences 49 (2010) 344‐351\n",
      "This is an exceptionally dense article! The author attempts to convince the reader that spatial\n",
      "intelligence is a test for STEM talent. Being spatial intelligence dominant, I would have agreed\n",
      "with that assertion before reading the paper. The paper serves to reinforce my personal\n",
      "understanding and empirical evidence of having spent many years in the field during which time\n",
      "I came to understand some of the best talent has a keen spatial intelligence.\n",
      "The importance of this paper to my project is simple: it confirms the STEM community is a\n",
      "perfect target audience.\n",
      "\n",
      "Conclusion\n",
      "This review is starting to get long. Rather than list additional reference materials, I’ll simply mention that\n",
      "I believe two other domains need to be, at least casually, researched and documented in my final\n",
      "delivery. They are animation and Realia. Both seem appropriate, but until I get further into the project,\n",
      "I’m not willing to say they are positively. I certainly don’t want to clutter this report with non‐essential\n",
      "materials.\n",
      "I view my project as the virtual equivalent of a working cutaway model of a machine. This machine, of\n",
      "course, lives in the virtual world, but it’s a machine nonetheless. Each of the three research areas\n",
      "outlined above will be critical to the success of my project.\n",
      "\n",
      "\f",
      "Assignment #5, CS6460 ET, Fall 2018, Chris Hockett (chockett3)\n",
      "References:\n",
      "Gardner H. (83) Frames of Mind ‐ The Theory of Multiple Intelligences. New York: Basic Books\n",
      "\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "original = textract.process(os.path.join(path, random_filename)).decode(\"utf-8\")\n",
    "print(original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assign_fall_chri_hockett chockett assign_collect_sourc introduct project intend creat tool visual process lexic_analysi exception profici tackl tool visual process grammat analysi clear piec stretch goal tool set like stimul learn appli spatial_intellig studi happen learner fit category‐ work behavior immedi understand i’m studi project requir research multipl area lexic_analysi graphic visual spatial_intellig associ best_practic teach learner mind annot_bibliographi subdivid relev categori lexic_analysi van engelen robert construct fast lexic_analyz flex retriev genivia websit http_www genivia com websit outlin lexer reflect tool closest imagin graph lexic_analysi state machin address visual mind‐ visual actual process input stream websit give import inform tool drill deepli actual mechan lexic_analysi inner work lexer valu project proof fact idea realiz brouwer gellerich ploedered myth fact effici implement finit automata lexic_analysi koskimi ed compil construct lectur_note_scienc vol springer_berlin_heidelberg paper interest read pertain measur perform number implement lexic_analysi algorithm real_world languag ada test case author evalu table‐driven hand‐craft scanner total algorithm analyz result particularli_interest assert table‐ driven analyz slower have goto branch unstructur code author show structur code lead_better perform optim assign_fall_chri_hockett chockett compil perform interest paper list lexic scanner gener awar exist derem lexic_analysi brauer ed compil construct lectur_note_scienc vol springer_berlin_heidelberg paper set chapter classic compendium lectur_note_scienc good introduct concept explor project classifi content rudimentari advanc depend chapter topic purpos text deal lexic_analysi held import easi read great refresh memori subtleti subject_matter lesk schmidt lex‐ lexic_analyz gener bell laboratori murrai hill new jersei paper outlin tool creat genr lexic_analyz generators‐ lex paper user manual sort explain produc process user’ input interfac compil gener tool paper_delv inner work tabl code fragment gener project revolv visual mechan tabl driven lexic_analyz requir deriv inform real valu paper descript us class tool histor_perspect neat trip time plan focu type analyz gener tool heur automat gener fast lexic analys depart electr engin univers colorado boulder colorado heur discuss “why’s” “how’s” automat lexic_analyz discuss revolv improv execut speed table‐driven analyz perform discuss paper interest primari appeal discuss recognit algorithm relev project‐ understand table‐driven analyz construct plan modifi algorithm produc supplement inform case visual data tabl finit_state machin intern machineri ru halverson unknown languag independ scanner gener depart scienc univers iowa iowa citi iowa ru halverson address altern implement lexic_analysi implement somewhat orthogon tradit table‐driven implement assign_fall_chri_hockett chockett us finit_state machin two‐level approach express recognit author provid detail explan method result attain valu paper project author type analyz intend explor time like work scanner gener algorithm fit schedul thompson regular express search algorithm commun associ_comput_machineri vol issu mcnaughton yamada regular express state graph automata ir transact electron comput vol ec‐ place item bibliographi potenti direct refer work consid foundat domain lexic_analyz referenc extens literatur fifti year older regular express typic mechan articul token try locat match formul strategi final paper_delv topic regular express discuss i’ll like refer paper don’t feel harm includ graphic visual graphviz current graph visual softwar retriev graphviz websit http_www graphviz org graphviz publicli_avail open‐sourc packag draw graph packag librari avail applic graphviz short graph visual us descript graph text languag creat correspond graph decor graph font color layout line style like differ layout engin design differ graph type size style primari packag render graph state machin tang simul transpar cutawai visual intern inform tangibl ui thesi massachusett institut_technolog tang’ thesi center concept appli tangibl ui computer‐aid design cad system proce discuss reason argument correct propos novel techniqu better interact application’ user casual observer’ perspect paper interest read topic relev_project assign_fall_chri_hockett chockett section detail cutawai view import numer exampl domain cutawai enhanc person’ understand relev_project propos implement cutawai view inner work lexic_analyz ritter agrawala curless salesin interact cutawai illustr complex model associ_comput_machineri transact graphic vol articl valu articl extens explor topic cutawai view applic understand best view user interact cutawai view author spend_time analyz primari domain mechan assembl anatom model includ argument need cutawai illustr support interact explor assert help user understand spatial relationship part kei_point project‐ believ user “see” insid analysi machineri gain greater understand part work unison token input stream ifenthal bridg_gap expert‐novic differ model‐bas feedback approach journal research technolog educ vol ifenthal devot entir paper make argument model‐bas feedback import compon learn understand specif paper introduc form model‐bas feedback‐ cutawai model discrep model ifenthal goe provid result improv comprehens student base newli introduc model don’t discrep model german project assert cutawai model improv comprehens relev argument cutawai model legitim form feedback learn import posit lidal hauser viola design_principl cutawai visual geolog model associ_comput_machineri ‐‐‐‐ sccg budmeric slovakia author explor depth design_principl cutawai visual admittedli earth scienc paper hold relev_project design_principl propos appli outlin principl_good cutawai model relev_project exampl illumin applic domain appli viewpoint simpl includ familiar context principl plan appli assign_fall_chri_hockett chockett spatial_intellig brualdi ami gardner’ theori teacher librarian nov dec brualdi review gardner’ theori multipl intellig gardner particular project spatial_intellig briefli person’ abil creat manipul mental imag solv_problem main argument paper educ focu multipl intellig poss instead educ broader view teach strength student’ skill assert test project aim stimul spatial_intellig learner visual simul agre author’ content folk learn better visual solut cooper eileen spatial‐tempor intellig origin think process gift inventor journal educ gift vol work focus argument spatial_intellig redefin spatial‐ tempor intellig ad time dimens spatial_intellig accept reason argument didn’t pick articl reason instead accur defin spatial_intellig contain follow skill visual imageri perceiv figur whole gener imag fragment understand spatial relationship multi‐perspect intern movement part it’ bit definit cornerston project‐ peopl simpli understand problem better complet visual intern movement part case part intern machineri lexic_analyz gardner reflect multipl intellig myth messag phi delta_kappan creator theori multipl intellig discuss debat stem origin theori gardner address myth set record straight i’m debat leav discuss theori defend assert struck applic_project educ set it’ wise attempt teach seven intellig read statement dawn tool propos said want clarifi posit best class folk best learn visual frankli grant tool practic us learn differ mean eliot spatial_intellig perceptu motor skill eliot describ multitud wai research subject spatial_intellig analysi research style littl applic_project leapt paper assert spatial_intellig contribut significantli technic scientif mathemat occup bolster posit tool like propos us class scientist pet project assign_fall_chri_hockett chockett newcomb frick earli educ spatial_intellig mind brain educ vol author lai case integr spatial content formal instruct articul number applic spatial_intellig popul gener particular discuss spatial_intellig scienc technolog_engin_mathemat stem_field spend fair time discuss mental rotat measur mental develop children follow observ suggest intervent necessari take‐awai paper implic student practic spatial skill classroom propos tool help özdener özcoban project_base learn model’ effect cours multipl intellig theori educ scienc theori practic author outlin research optim learn techniqu learner stronger intellig skill broke group student control sets‐ predomin intellig set blend intellig set determin tradit instruct better wors project_base instruct determin project_base instruct superior tradit explan base instruct furthermor group compris student differ domin intellig perform_better group member have singl domin intellig appli project tool design spatial_intellig domin student us project_base learn environ gardner hatch multipl intellig school educ implic theori multipl intellig educ research vol gardner hatch embark convers new approach assign human intellig test observ seven intellig school ag children observ student young exhibit strength_weak intellig spectrum furthermor social class plai factor develop intellig includ paper research test method instead import student understand intellig strength_weak know strength spatial_intellig like seek learn aid tool assist studi hegarti compon spatial_intellig psycholog learn motiv vol paper’ valu in‐depth definit spatial_intellig core spatial_intellig spatial think author’ word “the think shape arrang object space spatial process deform object movement object entiti space” page goe “it involv think spatial represent non‐spati entities” give exampl org chart think structur compani graph evalu chang assign_fall_chri_hockett chockett cost healthcar page discuss spatial think instruct school curriculum project support ad spatial think skill school curriculum prokysek rambousek widova research spatial_intellig effici applic spatial visual instruct procedia‐ social_behavior scienc paper look spatial_intellig ad spatial visual instruct boost student’ comprehens regardless domin intellig conclud increas student’ comprehens extens score subsequ test discuss type visual planer spatial author tackl subject type visual work best situat mention believ spatial visual main stream educ set belief visual aid contribut higher qualiti educ i’v look project vehicl domin spatial_intellig learner case benefit intellig type spatial visual lubinski spatial_abil stem sleep giant talent identif develop person individu differ exception dens articl author attempt convinc reader spatial_intellig test stem talent spatial_intellig domin agre assert read paper paper serv reinforc person understand empir_evid have spent year field time came understand best talent keen spatial_intellig import paper project simpl confirm stem commun perfect target_audienc conclus review start long list addit refer materi i’ll simpli mention believ domain need casual research document final deliveri anim realia appropri project i’m will posit certainli don’t want clutter report non‐essenti materi view project virtual equival work cutawai model machin machin cours live virtual_world it’ machin nonetheless research area outlin critic_success project assign_fall_chri_hockett chockett refer gardner frame mind theori multipl intellig new_york basic book\n"
     ]
    }
   ],
   "source": [
    "processed = trigram_assignments_list[random_index]\n",
    "print(processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working perfectly! Now, let's train Doc2Vec!\n",
    "\n",
    "# Training Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = []\n",
    "for file in os.listdir(path):\n",
    "    if assignment in str(file):\n",
    "        file_list.append(file)\n",
    "\n",
    "def get_student_name(file_index):\n",
    "    return file_list[file_index].split(\" -\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(fname, tokens_only=False):\n",
    "    with open(fname, encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if tokens_only:\n",
    "                yield gensim.utils.simple_preprocess(line)\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), [get_student_name(i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = list(read_corpus(trigram_assignments_filepath))\n",
    "test_corpus = list(read_corpus(trigram_assignments_filepath, tokens_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=300, min_count=5, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-06 17:51:43,325 : INFO : collecting all words and their counts\n",
      "2018-10-06 17:51:43,327 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2018-10-06 17:51:43,392 : INFO : collected 18055 word types and 184 unique tags from a corpus of 184 examples and 280049 words\n",
      "2018-10-06 17:51:43,393 : INFO : Loading a fresh vocabulary\n",
      "2018-10-06 17:51:43,456 : INFO : effective_min_count=5 retains 4736 unique words (26% of original 18055, drops 13319)\n",
      "2018-10-06 17:51:43,456 : INFO : effective_min_count=5 leaves 259371 word corpus (92% of original 280049, drops 20678)\n",
      "2018-10-06 17:51:43,479 : INFO : deleting the raw counts dictionary of 18055 items\n",
      "2018-10-06 17:51:43,482 : INFO : sample=0.001 downsamples 48 most-common words\n",
      "2018-10-06 17:51:43,485 : INFO : downsampling leaves estimated 235262 word corpus (90.7% of prior 259371)\n",
      "2018-10-06 17:51:43,505 : INFO : estimated required memory for 4736 words and 300 dimensions: 13992000 bytes\n",
      "2018-10-06 17:51:43,507 : INFO : resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "model.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-06 17:51:44,896 : INFO : training model with 3 workers on 4736 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-10-06 17:51:45,204 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:51:45,207 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:51:45,212 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:51:45,213 : INFO : EPOCH - 1 : training on 280049 raw words (235273 effective words) took 0.3s, 752419 effective words/s\n",
      "2018-10-06 17:51:45,472 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:51:45,475 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:51:45,489 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:51:45,489 : INFO : EPOCH - 2 : training on 280049 raw words (235303 effective words) took 0.3s, 855507 effective words/s\n",
      "2018-10-06 17:51:45,791 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:51:45,807 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:51:45,808 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:51:45,809 : INFO : EPOCH - 3 : training on 280049 raw words (235365 effective words) took 0.3s, 752833 effective words/s\n",
      "2018-10-06 17:51:46,111 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:51:46,119 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:51:46,120 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:51:46,121 : INFO : EPOCH - 4 : training on 280049 raw words (235359 effective words) took 0.3s, 761906 effective words/s\n",
      "2018-10-06 17:51:46,390 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:51:46,394 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:51:46,405 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:51:46,407 : INFO : EPOCH - 5 : training on 280049 raw words (235242 effective words) took 0.3s, 826409 effective words/s\n",
      "2018-10-06 17:51:46,745 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:51:46,753 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:51:46,758 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:51:46,759 : INFO : EPOCH - 6 : training on 280049 raw words (235538 effective words) took 0.4s, 672409 effective words/s\n",
      "2018-10-06 17:51:47,042 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:51:47,054 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:51:47,061 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:51:47,062 : INFO : EPOCH - 7 : training on 280049 raw words (235263 effective words) took 0.3s, 787076 effective words/s\n",
      "2018-10-06 17:51:47,384 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:51:47,407 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:51:47,416 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:51:47,417 : INFO : EPOCH - 8 : training on 280049 raw words (235488 effective words) took 0.4s, 668779 effective words/s\n",
      "2018-10-06 17:51:47,795 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:51:47,798 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:51:47,800 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:51:47,800 : INFO : EPOCH - 9 : training on 280049 raw words (235241 effective words) took 0.4s, 617856 effective words/s\n",
      "2018-10-06 17:51:48,139 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:51:48,148 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:51:48,149 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:51:48,149 : INFO : EPOCH - 10 : training on 280049 raw words (235319 effective words) took 0.3s, 678594 effective words/s\n",
      "2018-10-06 17:51:48,446 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:51:48,447 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:51:48,451 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:51:48,451 : INFO : EPOCH - 11 : training on 280049 raw words (235531 effective words) took 0.3s, 783986 effective words/s\n",
      "2018-10-06 17:51:48,891 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:51:48,895 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:51:48,904 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:51:48,904 : INFO : EPOCH - 12 : training on 280049 raw words (235476 effective words) took 0.4s, 523334 effective words/s\n",
      "2018-10-06 17:51:49,227 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:51:49,236 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:51:49,240 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:51:49,241 : INFO : EPOCH - 13 : training on 280049 raw words (235539 effective words) took 0.3s, 713969 effective words/s\n",
      "2018-10-06 17:51:49,542 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:51:49,547 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:51:49,548 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:51:49,548 : INFO : EPOCH - 14 : training on 280049 raw words (235449 effective words) took 0.3s, 777902 effective words/s\n",
      "2018-10-06 17:51:49,815 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:51:49,824 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:51:49,834 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:51:49,835 : INFO : EPOCH - 15 : training on 280049 raw words (235570 effective words) took 0.3s, 827945 effective words/s\n",
      "2018-10-06 17:51:50,117 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:51:50,124 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:51:50,126 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:51:50,127 : INFO : EPOCH - 16 : training on 280049 raw words (235501 effective words) took 0.3s, 814904 effective words/s\n",
      "2018-10-06 17:51:50,414 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:51:50,432 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:51:50,438 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:51:50,439 : INFO : EPOCH - 17 : training on 280049 raw words (235597 effective words) took 0.3s, 760180 effective words/s\n",
      "2018-10-06 17:51:50,772 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:51:50,781 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:51:50,786 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:51:50,787 : INFO : EPOCH - 18 : training on 280049 raw words (235556 effective words) took 0.3s, 684671 effective words/s\n",
      "2018-10-06 17:51:51,091 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:51:51,100 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:51:51,101 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:51:51,102 : INFO : EPOCH - 19 : training on 280049 raw words (235451 effective words) took 0.3s, 753953 effective words/s\n",
      "2018-10-06 17:51:51,378 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:51:51,384 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:51:51,391 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:51:51,391 : INFO : EPOCH - 20 : training on 280049 raw words (235450 effective words) took 0.3s, 818654 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-06 17:51:51,649 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:51:51,651 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:51:51,662 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:51:51,663 : INFO : EPOCH - 21 : training on 280049 raw words (235600 effective words) took 0.3s, 873778 effective words/s\n",
      "2018-10-06 17:51:51,927 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:51:51,946 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:51:51,949 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:51:51,949 : INFO : EPOCH - 22 : training on 280049 raw words (235443 effective words) took 0.3s, 828805 effective words/s\n",
      "2018-10-06 17:51:52,219 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:51:52,226 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:51:52,240 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:51:52,240 : INFO : EPOCH - 23 : training on 280049 raw words (235500 effective words) took 0.3s, 817083 effective words/s\n",
      "2018-10-06 17:51:52,497 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:51:52,507 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:51:52,512 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:51:52,512 : INFO : EPOCH - 24 : training on 280049 raw words (235418 effective words) took 0.3s, 879917 effective words/s\n",
      "2018-10-06 17:51:52,773 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:51:52,775 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:51:52,776 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:51:52,776 : INFO : EPOCH - 25 : training on 280049 raw words (235423 effective words) took 0.3s, 903941 effective words/s\n",
      "2018-10-06 17:51:53,046 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:51:53,054 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:51:53,056 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:51:53,057 : INFO : EPOCH - 26 : training on 280049 raw words (235405 effective words) took 0.3s, 847572 effective words/s\n",
      "2018-10-06 17:51:53,322 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:51:53,324 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:51:53,325 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:51:53,325 : INFO : EPOCH - 27 : training on 280049 raw words (235526 effective words) took 0.3s, 883857 effective words/s\n",
      "2018-10-06 17:51:53,633 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:51:53,642 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:51:53,655 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:51:53,656 : INFO : EPOCH - 28 : training on 280049 raw words (235464 effective words) took 0.3s, 718321 effective words/s\n",
      "2018-10-06 17:51:53,954 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:51:53,971 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:51:53,972 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:51:53,972 : INFO : EPOCH - 29 : training on 280049 raw words (235234 effective words) took 0.3s, 757611 effective words/s\n",
      "2018-10-06 17:51:54,288 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:51:54,296 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:51:54,296 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:51:54,297 : INFO : EPOCH - 30 : training on 280049 raw words (235419 effective words) took 0.3s, 731981 effective words/s\n",
      "2018-10-06 17:51:54,574 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:51:54,579 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:51:54,592 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:51:54,593 : INFO : EPOCH - 31 : training on 280049 raw words (235505 effective words) took 0.3s, 804018 effective words/s\n",
      "2018-10-06 17:51:54,915 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:51:54,922 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:51:54,929 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:51:54,930 : INFO : EPOCH - 32 : training on 280049 raw words (235450 effective words) took 0.3s, 702307 effective words/s\n",
      "2018-10-06 17:51:55,248 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:51:55,253 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:51:55,262 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:51:55,263 : INFO : EPOCH - 33 : training on 280049 raw words (235466 effective words) took 0.3s, 710360 effective words/s\n",
      "2018-10-06 17:51:55,527 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:51:55,529 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:51:55,531 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:51:55,531 : INFO : EPOCH - 34 : training on 280049 raw words (235482 effective words) took 0.3s, 885516 effective words/s\n",
      "2018-10-06 17:51:55,810 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:51:55,811 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:51:55,822 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:51:55,822 : INFO : EPOCH - 35 : training on 280049 raw words (235487 effective words) took 0.3s, 815321 effective words/s\n",
      "2018-10-06 17:51:56,138 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:51:56,138 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:51:56,153 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:51:56,153 : INFO : EPOCH - 36 : training on 280049 raw words (235367 effective words) took 0.3s, 715528 effective words/s\n",
      "2018-10-06 17:51:56,424 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:51:56,425 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:51:56,427 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:51:56,427 : INFO : EPOCH - 37 : training on 280049 raw words (235524 effective words) took 0.3s, 871846 effective words/s\n",
      "2018-10-06 17:51:56,710 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:51:56,717 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:51:56,718 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:51:56,718 : INFO : EPOCH - 38 : training on 280049 raw words (235546 effective words) took 0.3s, 814588 effective words/s\n",
      "2018-10-06 17:51:57,010 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:51:57,025 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:51:57,026 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:51:57,026 : INFO : EPOCH - 39 : training on 280049 raw words (235225 effective words) took 0.3s, 767492 effective words/s\n",
      "2018-10-06 17:51:57,327 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:51:57,333 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:51:57,344 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:51:57,344 : INFO : EPOCH - 40 : training on 280049 raw words (235217 effective words) took 0.3s, 745934 effective words/s\n",
      "2018-10-06 17:51:57,651 : INFO : worker thread finished; awaiting finish of 2 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-06 17:51:57,658 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:51:57,672 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:51:57,673 : INFO : EPOCH - 41 : training on 280049 raw words (235596 effective words) took 0.3s, 722080 effective words/s\n",
      "2018-10-06 17:51:57,951 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:51:57,962 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:51:57,965 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:51:57,966 : INFO : EPOCH - 42 : training on 280049 raw words (235481 effective words) took 0.3s, 810750 effective words/s\n",
      "2018-10-06 17:51:58,237 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:51:58,239 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:51:58,241 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:51:58,241 : INFO : EPOCH - 43 : training on 280049 raw words (235297 effective words) took 0.3s, 859697 effective words/s\n",
      "2018-10-06 17:51:58,546 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:51:58,547 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:51:58,558 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:51:58,559 : INFO : EPOCH - 44 : training on 280049 raw words (235490 effective words) took 0.3s, 746561 effective words/s\n",
      "2018-10-06 17:51:58,826 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:51:58,826 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:51:58,836 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:51:58,837 : INFO : EPOCH - 45 : training on 280049 raw words (235380 effective words) took 0.3s, 853285 effective words/s\n",
      "2018-10-06 17:51:59,139 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:51:59,142 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:51:59,144 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:51:59,145 : INFO : EPOCH - 46 : training on 280049 raw words (235363 effective words) took 0.3s, 768832 effective words/s\n",
      "2018-10-06 17:51:59,504 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:51:59,505 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:51:59,516 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:51:59,517 : INFO : EPOCH - 47 : training on 280049 raw words (235408 effective words) took 0.4s, 638237 effective words/s\n",
      "2018-10-06 17:51:59,793 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:51:59,806 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:51:59,807 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:51:59,807 : INFO : EPOCH - 48 : training on 280049 raw words (235499 effective words) took 0.3s, 816042 effective words/s\n",
      "2018-10-06 17:52:00,076 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:52:00,079 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:52:00,089 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:52:00,090 : INFO : EPOCH - 49 : training on 280049 raw words (235453 effective words) took 0.3s, 841470 effective words/s\n",
      "2018-10-06 17:52:00,402 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:52:00,403 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:52:00,407 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:52:00,408 : INFO : EPOCH - 50 : training on 280049 raw words (235496 effective words) took 0.3s, 743812 effective words/s\n",
      "2018-10-06 17:52:00,688 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:52:00,691 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:52:00,696 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:52:00,697 : INFO : EPOCH - 51 : training on 280049 raw words (235571 effective words) took 0.3s, 829455 effective words/s\n",
      "2018-10-06 17:52:00,992 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:52:00,997 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:52:01,002 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:52:01,002 : INFO : EPOCH - 52 : training on 280049 raw words (235352 effective words) took 0.3s, 777279 effective words/s\n",
      "2018-10-06 17:52:01,265 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:52:01,270 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:52:01,276 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:52:01,276 : INFO : EPOCH - 53 : training on 280049 raw words (235690 effective words) took 0.3s, 867080 effective words/s\n",
      "2018-10-06 17:52:01,574 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:52:01,576 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:52:01,579 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:52:01,579 : INFO : EPOCH - 54 : training on 280049 raw words (235449 effective words) took 0.3s, 781309 effective words/s\n",
      "2018-10-06 17:52:01,868 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:52:01,870 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:52:01,873 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:52:01,873 : INFO : EPOCH - 55 : training on 280049 raw words (235441 effective words) took 0.3s, 806808 effective words/s\n",
      "2018-10-06 17:52:02,108 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:52:02,116 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:52:02,126 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:52:02,127 : INFO : EPOCH - 56 : training on 280049 raw words (235349 effective words) took 0.3s, 936031 effective words/s\n",
      "2018-10-06 17:52:02,403 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:52:02,409 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:52:02,419 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:52:02,420 : INFO : EPOCH - 57 : training on 280049 raw words (235461 effective words) took 0.3s, 814238 effective words/s\n",
      "2018-10-06 17:52:02,691 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:52:02,698 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:52:02,699 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:52:02,700 : INFO : EPOCH - 58 : training on 280049 raw words (235417 effective words) took 0.3s, 845294 effective words/s\n",
      "2018-10-06 17:52:02,965 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:52:02,967 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:52:02,974 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:52:02,974 : INFO : EPOCH - 59 : training on 280049 raw words (235419 effective words) took 0.3s, 863953 effective words/s\n",
      "2018-10-06 17:52:03,286 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:52:03,294 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:52:03,308 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:52:03,309 : INFO : EPOCH - 60 : training on 280049 raw words (235500 effective words) took 0.3s, 710356 effective words/s\n",
      "2018-10-06 17:52:03,561 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:52:03,563 : INFO : worker thread finished; awaiting finish of 1 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-06 17:52:03,568 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:52:03,568 : INFO : EPOCH - 61 : training on 280049 raw words (235520 effective words) took 0.3s, 914488 effective words/s\n",
      "2018-10-06 17:52:03,812 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:52:03,819 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:52:03,821 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:52:03,823 : INFO : EPOCH - 62 : training on 280049 raw words (235372 effective words) took 0.3s, 930516 effective words/s\n",
      "2018-10-06 17:52:04,127 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:52:04,131 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:52:04,144 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:52:04,144 : INFO : EPOCH - 63 : training on 280049 raw words (235318 effective words) took 0.3s, 741845 effective words/s\n",
      "2018-10-06 17:52:04,425 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:52:04,426 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:52:04,430 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:52:04,431 : INFO : EPOCH - 64 : training on 280049 raw words (235573 effective words) took 0.3s, 825854 effective words/s\n",
      "2018-10-06 17:52:04,731 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:52:04,739 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:52:04,743 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:52:04,743 : INFO : EPOCH - 65 : training on 280049 raw words (235399 effective words) took 0.3s, 759764 effective words/s\n",
      "2018-10-06 17:52:05,018 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:52:05,023 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:52:05,034 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:52:05,035 : INFO : EPOCH - 66 : training on 280049 raw words (235456 effective words) took 0.3s, 825125 effective words/s\n",
      "2018-10-06 17:52:05,280 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:52:05,283 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:52:05,287 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:52:05,287 : INFO : EPOCH - 67 : training on 280049 raw words (235348 effective words) took 0.2s, 943425 effective words/s\n",
      "2018-10-06 17:52:05,576 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:52:05,577 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:52:05,584 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:52:05,584 : INFO : EPOCH - 68 : training on 280049 raw words (235459 effective words) took 0.3s, 798046 effective words/s\n",
      "2018-10-06 17:52:05,897 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:52:05,902 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:52:05,909 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:52:05,910 : INFO : EPOCH - 69 : training on 280049 raw words (235403 effective words) took 0.3s, 727749 effective words/s\n",
      "2018-10-06 17:52:06,180 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:52:06,196 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:52:06,197 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:52:06,197 : INFO : EPOCH - 70 : training on 280049 raw words (235361 effective words) took 0.3s, 824157 effective words/s\n",
      "2018-10-06 17:52:06,500 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:52:06,501 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:52:06,510 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:52:06,510 : INFO : EPOCH - 71 : training on 280049 raw words (235640 effective words) took 0.3s, 758845 effective words/s\n",
      "2018-10-06 17:52:06,796 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:52:06,799 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:52:06,804 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:52:06,805 : INFO : EPOCH - 72 : training on 280049 raw words (235418 effective words) took 0.3s, 812487 effective words/s\n",
      "2018-10-06 17:52:07,068 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:52:07,085 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:52:07,091 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:52:07,092 : INFO : EPOCH - 73 : training on 280049 raw words (235421 effective words) took 0.3s, 828867 effective words/s\n",
      "2018-10-06 17:52:07,362 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:52:07,370 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:52:07,374 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:52:07,374 : INFO : EPOCH - 74 : training on 280049 raw words (235320 effective words) took 0.3s, 845870 effective words/s\n",
      "2018-10-06 17:52:07,650 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:52:07,651 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:52:07,663 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:52:07,664 : INFO : EPOCH - 75 : training on 280049 raw words (235557 effective words) took 0.3s, 821105 effective words/s\n",
      "2018-10-06 17:52:07,981 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:52:07,983 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:52:07,987 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:52:07,988 : INFO : EPOCH - 76 : training on 280049 raw words (235445 effective words) took 0.3s, 732285 effective words/s\n",
      "2018-10-06 17:52:08,217 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:52:08,225 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:52:08,234 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:52:08,235 : INFO : EPOCH - 77 : training on 280049 raw words (235315 effective words) took 0.2s, 962126 effective words/s\n",
      "2018-10-06 17:52:08,480 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:52:08,491 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:52:08,494 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:52:08,495 : INFO : EPOCH - 78 : training on 280049 raw words (235284 effective words) took 0.3s, 924105 effective words/s\n",
      "2018-10-06 17:52:08,803 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:52:08,806 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:52:08,812 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:52:08,813 : INFO : EPOCH - 79 : training on 280049 raw words (235505 effective words) took 0.3s, 749416 effective words/s\n",
      "2018-10-06 17:52:09,057 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:52:09,058 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:52:09,064 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:52:09,064 : INFO : EPOCH - 80 : training on 280049 raw words (235451 effective words) took 0.2s, 944643 effective words/s\n",
      "2018-10-06 17:52:09,304 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:52:09,309 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:52:09,311 : INFO : worker thread finished; awaiting finish of 0 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-06 17:52:09,311 : INFO : EPOCH - 81 : training on 280049 raw words (235475 effective words) took 0.2s, 961143 effective words/s\n",
      "2018-10-06 17:52:09,561 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:52:09,566 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:52:09,571 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:52:09,572 : INFO : EPOCH - 82 : training on 280049 raw words (235481 effective words) took 0.3s, 910550 effective words/s\n",
      "2018-10-06 17:52:09,804 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:52:09,806 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:52:09,809 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:52:09,809 : INFO : EPOCH - 83 : training on 280049 raw words (235408 effective words) took 0.2s, 1001668 effective words/s\n",
      "2018-10-06 17:52:10,050 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:52:10,057 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:52:10,065 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:52:10,066 : INFO : EPOCH - 84 : training on 280049 raw words (235553 effective words) took 0.3s, 928048 effective words/s\n",
      "2018-10-06 17:52:10,306 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:52:10,315 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:52:10,322 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:52:10,322 : INFO : EPOCH - 85 : training on 280049 raw words (235417 effective words) took 0.3s, 929484 effective words/s\n",
      "2018-10-06 17:52:10,553 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:52:10,555 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:52:10,560 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:52:10,561 : INFO : EPOCH - 86 : training on 280049 raw words (235596 effective words) took 0.2s, 1003251 effective words/s\n",
      "2018-10-06 17:52:10,791 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:52:10,796 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:52:10,806 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:52:10,807 : INFO : EPOCH - 87 : training on 280049 raw words (235397 effective words) took 0.2s, 961961 effective words/s\n",
      "2018-10-06 17:52:11,041 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:52:11,050 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:52:11,052 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:52:11,053 : INFO : EPOCH - 88 : training on 280049 raw words (235367 effective words) took 0.2s, 973411 effective words/s\n",
      "2018-10-06 17:52:11,283 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:52:11,294 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:52:11,296 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:52:11,296 : INFO : EPOCH - 89 : training on 280049 raw words (235402 effective words) took 0.2s, 978650 effective words/s\n",
      "2018-10-06 17:52:11,542 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:52:11,547 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:52:11,558 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:52:11,559 : INFO : EPOCH - 90 : training on 280049 raw words (235391 effective words) took 0.3s, 902079 effective words/s\n",
      "2018-10-06 17:52:11,786 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:52:11,797 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:52:11,806 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:52:11,807 : INFO : EPOCH - 91 : training on 280049 raw words (235406 effective words) took 0.2s, 991816 effective words/s\n",
      "2018-10-06 17:52:12,051 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:52:12,055 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:52:12,062 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:52:12,062 : INFO : EPOCH - 92 : training on 280049 raw words (235262 effective words) took 0.3s, 937294 effective words/s\n",
      "2018-10-06 17:52:12,295 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:52:12,300 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:52:12,305 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:52:12,305 : INFO : EPOCH - 93 : training on 280049 raw words (235531 effective words) took 0.2s, 976447 effective words/s\n",
      "2018-10-06 17:52:12,532 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:52:12,541 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:52:12,551 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:52:12,552 : INFO : EPOCH - 94 : training on 280049 raw words (235466 effective words) took 0.2s, 961292 effective words/s\n",
      "2018-10-06 17:52:12,791 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:52:12,793 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:52:12,797 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:52:12,797 : INFO : EPOCH - 95 : training on 280049 raw words (235545 effective words) took 0.2s, 977531 effective words/s\n",
      "2018-10-06 17:52:13,035 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:52:13,043 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:52:13,049 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:52:13,049 : INFO : EPOCH - 96 : training on 280049 raw words (235510 effective words) took 0.2s, 943009 effective words/s\n",
      "2018-10-06 17:52:13,310 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:52:13,317 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:52:13,317 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:52:13,318 : INFO : EPOCH - 97 : training on 280049 raw words (235479 effective words) took 0.3s, 885903 effective words/s\n",
      "2018-10-06 17:52:13,551 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:52:13,554 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:52:13,568 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:52:13,568 : INFO : EPOCH - 98 : training on 280049 raw words (235412 effective words) took 0.2s, 950851 effective words/s\n",
      "2018-10-06 17:52:13,813 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:52:13,817 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:52:13,822 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:52:13,822 : INFO : EPOCH - 99 : training on 280049 raw words (235530 effective words) took 0.3s, 941759 effective words/s\n",
      "2018-10-06 17:52:14,068 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-06 17:52:14,076 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-06 17:52:14,079 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-06 17:52:14,079 : INFO : EPOCH - 100 : training on 280049 raw words (235568 effective words) took 0.3s, 924114 effective words/s\n",
      "2018-10-06 17:52:14,080 : INFO : training on a 28004900 raw words (23544085 effective words) took 29.2s, 806815 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 12s, sys: 844 ms, total: 1min 13s\n",
      "Wall time: 29.2 s\n"
     ]
    }
   ],
   "source": [
    "%time model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-06 17:52:27,602 : INFO : saving Doc2Vec object under /Users/carlossouza/Dropbox/playground/gensim/pdfs/doc2vec_model_assignment_5, separately None\n",
      "2018-10-06 17:52:27,750 : INFO : saved /Users/carlossouza/Dropbox/playground/gensim/pdfs/doc2vec_model_assignment_5\n"
     ]
    }
   ],
   "source": [
    "doc2vec_model_filepath = os.path.join(path, 'doc2vec_model_' + assignment_code)\n",
    "if save_files:\n",
    "    model.save(doc2vec_model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking the result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_student = \"Carlos Souza\"\n",
    "test_student_index = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-06 17:52:33,255 : INFO : precomputing L2-norms of doc weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar documents of Carlos Souza for Assignment 5\n",
      "0.3223743438720703 \tAbdiel Sanchez\n",
      "0.2908666133880615 \tSteven Gray\n",
      "0.28265196084976196 \tAlaa Shafaee\n",
      "0.2778726816177368 \tChristine Mcmanus\n",
      "0.25868552923202515 \tNavnit Belur\n",
      "0.2560320496559143 \tSayed Ali\n",
      "0.2442658394575119 \tBhaskar Majji\n",
      "0.23750287294387817 \tAravind Rajamani\n",
      "0.23571227490901947 \tZeya Aung\n",
      "0.2298918217420578 \tTerrence Chida\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "similar_doc = model.docvecs.most_similar(test_student) \n",
    "print(\"Most similar documents of \" + test_student + \" for \" + assignment)\n",
    "for item in similar_doc:\n",
    "    print(str(item[1])  + \" \\t\" + str(item[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verifying Topics with TextRazor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textrazor\n",
    "\n",
    "textrazor.api_key = \"d26228c7df9e0ea2a9c2656b520eefe049ae8c837fb1ef5a95619e04\"\n",
    "client = textrazor.TextRazor(extractors=[\"entities\", \"topics\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_entities(student, limit=15):\n",
    "    text = textract.process(os.path.join(path, student + \" - \" + assignment + \".pdf\")).decode(\"utf-8\")\n",
    "    response = client.analyze(text)\n",
    "    entities = list(response.entities())\n",
    "    entities.sort(key=lambda x: x.relevance_score, reverse=True)\n",
    "    seen = set()\n",
    "    index = 0\n",
    "    for entity in entities:\n",
    "        if entity.id not in seen:\n",
    "            print(entity.id, entity.relevance_score, entity.confidence_score)\n",
    "            seen.add(entity.id)\n",
    "            index += 1\n",
    "            if index >= limit:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implicit theories of intelligence 1 3.858\n",
      "Grit (personality trait) 1 2.91\n",
      "Praise 1 2.898\n",
      "Practice (learning method) 1 1.722\n",
      "Constructivism (philosophy of education) 1 13.52\n",
      "Educational technology 0.9193 10.65\n",
      "Carol Dweck 0.9027 2.579\n",
      "Problem solving 0.8731 8.915\n",
      "Internet bot 0.8699 3.106\n",
      "Cognitive tutor 0.8641 5.617\n",
      "Big Five personality traits 0.8641 2.311\n",
      "Adaptive learning 0.862 2.434\n",
      "Self-regulated learning 0.8504 6.654\n",
      "Motivation 0.8363 5.049\n",
      "Chatbot 0.8359 9.671\n"
     ]
    }
   ],
   "source": [
    "print_top_entities(\"Carlos Souza\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Social cognitive theory 1 1.827\n",
      "Self-efficacy 1 11.25\n",
      "Problem solving 1 9.216\n",
      "Technology integration 1 1.396\n",
      "National Council of Teachers of Mathematics 1 5.684\n",
      "Self-confidence 0.9839 3.844\n",
      "Albert Bandura 0.9664 1.417\n",
      "Qualitative research 0.9602 4.475\n",
      "Educational technology 0.9587 4.621\n",
      "Carol Dweck 0.9412 4.922\n",
      "Instructional scaffolding 0.9224 2.379\n",
      "Motivation 0.9167 5.581\n",
      "Numeracy 0.9046 7.64\n",
      "Science, technology, engineering, and mathematics 0.8968 7.094\n",
      "Gender role 0.8842 1.342\n"
     ]
    }
   ],
   "source": [
    "print_top_entities(\"Christine Mcmanus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further reading:\n",
    "https://cs.stanford.edu/~quocle/paragraph_vector.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
