{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Using Doc2Vec to calculate similarity between documents\n",
    "\n",
    "## Overview\n",
    "Duration: 1min\n",
    "\n",
    "This tutorial will show you how to use Doc2Vec to calculate similarity scores between pairs of documents. Doc2Vec is an NLP tool for representing documents as a vector and is a generalizing of the Word2Vec method. \n",
    "\n",
    "Before you start, follow the instructions from [README.md](https://github.com/ucals/bettertogether/blob/master/README.md) to get the PDF documents in your local machine. This corpus, which will be located in `pdfs/` folder, contains ~1,000 documents. They are essays written by OMSCS CS-6460 students in Fall 2018 class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What you'll learn\n",
    "\n",
    "- How to pre-process text, getting it ready to apply NLP tecniques, by using [Gensim Library](https://radimrehurek.com/gensim/)\n",
    "- How to train Doc2Vec model, also using [Gensim Library](https://radimrehurek.com/gensim/)\n",
    "- How to test it by eye, using [TextRazor](https://www.textrazor.com/) classification API to check if the similarities from our trained model are actually true\n",
    "\n",
    "### What you'll need\n",
    "\n",
    "- The corpora: a collection of PDF files correspondent to Assignments 2, 3 and 4 from OMSCS CS-6460 students\n",
    "- Some basic Python knowledge\n",
    "\n",
    "### Steps:\n",
    "\n",
    "1. Preprocess the text: 10min\n",
    "2. Train Doc2Vec model: 5min\n",
    "3. Test the result: 10min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Preprocess the text\n",
    "\n",
    "The first step will be preprocess the text, getting it ready to train Doc2Vec model. We will do this by combining all assignment documents in one single file called `unigram_sentences_all_assignment_X.txt`, where X will be 2, 3 or 4 depending on the assignment. In this file, each line will be one sentence from the assignment documents. We will read the assignment documents line-by-line, pre-process each line using a simple gensim pre-processing tool (i.e., tokenize text into individual words, remove punctuation, set to lowercase, etc), and return a list of words. Then, we will combine the list of words of each line in one string (separated by spaces) and store it in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic setup\n",
    "import logging, gensim, os, textract\n",
    "import itertools as it\n",
    "from gensim.parsing.preprocessing import preprocess_string, remove_stopwords\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "assignment = \"Assignment 2\"\n",
    "save_files = True\n",
    "\n",
    "assignment_code = assignment.lower().replace(' ', '_')\n",
    "path = os.path.join(os.getcwd(), \"pdfs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_sentences_filepath = os.path.join(path, 'unigram_sentences_all_' + assignment_code + '.txt')\n",
    "\n",
    "if save_files:\n",
    "    with open(unigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "        for file in os.listdir(path):\n",
    "            if assignment in str(file):\n",
    "                text = textract.process(os.path.join(path, file)).decode(\"utf-8\")\n",
    "                unigram_sentences = []\n",
    "                for sent in gensim.summarization.textcleaner.get_sentences(text):\n",
    "                    processed = preprocess_string(sent)\n",
    "                    if len(processed) > 0:\n",
    "                        f.write(\" \".join(processed) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim's LineSentence class provides a convenient iterator for working with other gensim components. It streams the documents/sentences from disk, so that you never have to hold the entire corpus in RAM at once. This allows you to scale your modeling pipeline up to potentially very large corpora:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_sentences = gensim.models.word2vec.LineSentence(unigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to apply *phrase modeling* to combine tokens that together represent meaningful multi-word concepts (bi-grams and tri-grams). After applying it, `new york` would become `new_york`; `new york times` would become `new_york_times`. \n",
    "\n",
    "First, let's create bi-grams from the uni-grams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-11 23:44:57,631 : INFO : collecting all words and their counts\n",
      "2018-10-11 23:44:57,634 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2018-10-11 23:44:57,743 : INFO : PROGRESS: at sentence #10000, processed 45671 words and 31392 word types\n",
      "2018-10-11 23:44:57,827 : INFO : collected 53046 word types from a corpus of 86396 words (unigram + bigrams) and 18830 sentences\n",
      "2018-10-11 23:44:57,827 : INFO : using 53046 counts as vocab in Phrases<0 vocab, min_count=5, threshold=10.0, max_vocab_size=40000000>\n",
      "2018-10-11 23:44:57,828 : INFO : saving Phrases object under /Users/carlossouza/Dropbox/2018/OMSCS/CS-6460-Education-Technology/bettertogether/pdfs/bigram_model_all_assignment_2, separately None\n",
      "2018-10-11 23:44:57,904 : INFO : saved /Users/carlossouza/Dropbox/2018/OMSCS/CS-6460-Education-Technology/bettertogether/pdfs/bigram_model_all_assignment_2\n"
     ]
    }
   ],
   "source": [
    "bigram_model_filepath = os.path.join(path, 'bigram_model_all_' + assignment_code)\n",
    "bigram_model = gensim.models.Phrases(unigram_sentences)\n",
    "if save_files:\n",
    "    bigram_model.save(bigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "bigram_sentences_filepath = os.path.join(path, 'bigram_sentences_all_' + assignment_code + '.txt')\n",
    "\n",
    "if save_files:\n",
    "    with open(bigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "        for unigram_sentence in unigram_sentences:\n",
    "            bigram_sentence = \" \".join(bigram_model[unigram_sentence])\n",
    "            f.write(bigram_sentence + \"\\n\")\n",
    "            \n",
    "bigram_sentences = gensim.models.word2vec.LineSentence(bigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that it worked by examining a small slice of our file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inform tent project cours onlin languag exam app\n",
      "\n",
      "gener work field languag math_scienc\n",
      "\n",
      "mobil_devic\n",
      "\n",
      "mobil learn fellow text http blog acu edu adamscent fund mobilelearn fellow present research abilen christian univers\n",
      "\n",
      "interest total unexpect result phone facebook usag\n",
      "\n",
      "correl spiritu\n",
      "\n",
      "excess social comput associ decreas religi wellb overal life\n",
      "\n",
      "satisfact\n",
      "\n",
      "person reloc origin home countri facebook social_media\n",
      "\n",
      "wai mean keep relat old friend famili think\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bigram_sentences = gensim.models.word2vec.LineSentence(bigram_sentences_filepath)\n",
    "\n",
    "for bigram_sentence in it.islice(bigram_sentences, 3000, 3010):\n",
    "    print(u' '.join(bigram_sentence))\n",
    "    print(u'')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the algorithm combined some tokens, forming bi-grams like `mobil_devic`, `social_media`, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use the bi-grams to create tri-grams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-11 23:46:33,069 : INFO : collecting all words and their counts\n",
      "2018-10-11 23:46:33,071 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2018-10-11 23:46:33,173 : INFO : PROGRESS: at sentence #10000, processed 43031 words and 31843 word types\n",
      "2018-10-11 23:46:33,248 : INFO : collected 54074 word types from a corpus of 81319 words (unigram + bigrams) and 18830 sentences\n",
      "2018-10-11 23:46:33,248 : INFO : using 54074 counts as vocab in Phrases<0 vocab, min_count=5, threshold=10.0, max_vocab_size=40000000>\n",
      "2018-10-11 23:46:33,249 : INFO : saving Phrases object under /Users/carlossouza/Dropbox/2018/OMSCS/CS-6460-Education-Technology/bettertogether/pdfs/trigram_model_all_assignment_2, separately None\n",
      "2018-10-11 23:46:33,317 : INFO : saved /Users/carlossouza/Dropbox/2018/OMSCS/CS-6460-Education-Technology/bettertogether/pdfs/trigram_model_all_assignment_2\n"
     ]
    }
   ],
   "source": [
    "trigram_model_filepath = os.path.join(path, 'trigram_model_all_' + assignment_code)\n",
    "trigram_model = gensim.models.Phrases(bigram_sentences)\n",
    "if save_files:\n",
    "    trigram_model.save(trigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "trigram_sentences_filepath = os.path.join(path, 'trigram_sentences_all_' + assignment_code + '.txt')\n",
    "\n",
    "if save_files:\n",
    "    with open(trigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "        for bigram_sentence in bigram_sentences:\n",
    "            trigram_sentence = \" \".join(trigram_model[bigram_sentence])\n",
    "            f.write(trigram_sentence + \"\\n\")\n",
    "            \n",
    "trigram_sentences = gensim.models.word2vec.LineSentence(trigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "trigram_assignments_filepath = os.path.join(path, 'trigram_transformed_assignments_all_' + assignment_code + '.txt')\n",
    "\n",
    "if save_files:\n",
    "    with open(trigram_assignments_filepath, 'w', encoding='utf_8') as f:\n",
    "        for file in os.listdir(path):\n",
    "            if assignment in str(file):\n",
    "                text = textract.process(os.path.join(path, file)).decode(\"utf-8\")\n",
    "                unigram_assignment = preprocess_string(text)\n",
    "                bigram_assignment = bigram_model[unigram_assignment]\n",
    "                trigram_assignment = trigram_model[bigram_assignment]\n",
    "                f.write(\" \".join(trigram_assignment) + \"\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine a slice of our tri-gram file to check that it worked as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huhn\n",
      "\n",
      "don’t topic\n",
      "\n",
      "develop game believ game teach great design\n",
      "\n",
      "implement challeng\n",
      "\n",
      "simul_base_learn interest allow student experi environ\n",
      "\n",
      "abl experi\n",
      "\n",
      "provid wai experi\n",
      "\n",
      "activ danger time student fly airplan\n",
      "\n",
      "note learn theori “simul base_learn constructivist learn model\n",
      "\n",
      "provid learner experi work usual simplifi simul world\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for trigram_sentence in it.islice(trigram_sentences, 1000, 1010):\n",
    "    print(u' '.join(trigram_sentence))\n",
    "    print(u'')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it worked: our algorithm created tri-grams like `simul_base_learn` (simulated based learning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the original version with the preprocessed version of an assignment\n",
    "\n",
    "Let's see exactly what we did by randomly selecting an assignment and comparing as slice (first 2000 characters) of its original version with an slice of its preprocessed version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "\n",
    "file_list = []\n",
    "for file in os.listdir(path):\n",
    "    if assignment in str(file):\n",
    "        file_list.append(file)\n",
    "\n",
    "random_index = randrange(len(file_list))\n",
    "random_filename = file_list[random_index]\n",
    "\n",
    "with open(trigram_assignments_filepath) as f:\n",
    "    trigram_assignments_list = f.readlines()\n",
    "\n",
    "trigram_assignments_list = [x.strip() for x in trigram_assignments_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tyler Roland\n",
      "CS 6460\n",
      "Assignment 2\n",
      "\n",
      "Learning Management Systems (LMS) are a powerful educational tool that can be used by\n",
      "numerous fields, from education to medicine to software and technology. They incorporate different\n",
      "components to help facilitate learning that can be modified, scaled, and directed to different audiences,\n",
      "all while using the same program. For example, schools can choose to use LMS systems to help their\n",
      "students keep track of assignments for each class, communicate with their peers and professors,\n",
      "monitor grades and academic progress, and access learning tools such as a school library or other\n",
      "important documents. Canvas and T-Square are examples of LMS systems that are currently being used\n",
      "by Georgia Tech. Software and technology professionals can include other types of LMS systems to help\n",
      "train companies and users on how to use their software. There are many types of learning management\n",
      "systems around today, and each has their advantages and disadvantages, but there are still general\n",
      "improvements that can be added to keep them up-to-date with the advancement of technology.\n",
      "Online learning is a quickly growing field. Many schools are incorporating online learning to\n",
      "include more students that wouldn’t otherwise be able to attend due to time or travel constraints, and\n",
      "to allow collaboration with individuals around the world. Students in the OMSCS program at Georgia\n",
      "Tech experience the benefits of online learning first-hand. Some of the advantages of online learning are\n",
      "self-paced learning because course content or training content can be viewed and reviewed at any time\n",
      "of day, increased access to classes that are unavailable in person due to understaffing or high labor cost,\n",
      "and a higher quality of learning because different tools can be adapted to different learning styles and\n",
      "links and references to other helpful works or websites can be accessed right away with the click of a\n",
      "button. Personally, I highly enjoy online learning because of these be\n"
     ]
    }
   ],
   "source": [
    "original = textract.process(os.path.join(path, random_filename)).decode(\"utf-8\")\n",
    "print(original[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tyler roland assign learn_manag_system lm power educ tool numer field educ medicin softwar technolog incorpor differ compon help facilit learn modifi scale direct differ audienc program exampl school choos us lm system help student track assign class commun peer professor monitor grade academ progress access learn tool school librari import document canva squar exampl lm system current georgia_tech softwar technolog profession includ type lm system help train compani user us softwar type learn_manag_system todai advantag disadvantag gener improv ad date advanc technolog onlin learn quickli grow field school incorpor onlin learn includ student wouldn’t abl attend time travel constraint allow collabor individu world student omsc_program georgia_tech experi benefit onlin learn hand advantag onlin learn self pace learn cours content train content view review time dai increas access class unavail person understaf high labor cost higher qualiti learn differ tool adapt differ learn style link refer help work websit access right awai click button person highli enjoi onlin learn benefit attend colleg person bachelor’ degre attest neg argument onlin learn aim revers need specif class offer earli morn need class wasn’t offer semest want big thing dislik person learn onlin learn doesn’t affect educ compani us onlin learn offer train staff pace time cut wast work time allow inform access time futur necessari massiv_open_onlin_cours mooc similar regular onlin learn cater larg unlimit number student initi sound like great benefit david_joyner state “introduct moocs” video lectur educ_technolog class import constraint develop mooc abl creat good qualiti content assess peopl have spend lot monei product qualiti hire teach assist help question grade limit attend class student thousand student abl access inform feedback possibl time known platform offer mooc coursera udemi exampl lm system person familiar know udac origin design mooc platform avail mooc class benefici class differ ins\n"
     ]
    }
   ],
   "source": [
    "processed = trigram_assignments_list[random_index]\n",
    "print(processed[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, our algorithm worked perfectly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Train Doc2Vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we train our model, let's define a function `read_corpus` to open the train/test file (with latin encoding), read the file line-by-line, pre-process each line using a simple gensim pre-processing tool (i.e., tokenize text into individual words, remove punctuation, set to lowercase, etc), and return a list of words. Also, to train the model, we'll need to associate a tag/number with each line of the training corpus. In our case, the tag is the student's name, given by `get_student_name` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(fname, tokens_only=False):\n",
    "    with open(fname, encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if tokens_only:\n",
    "                yield gensim.utils.simple_preprocess(line)\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), [get_student_name(i)])\n",
    "                \n",
    "def get_student_name(file_index):\n",
    "    return file_list[file_index].split(\" -\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = list(read_corpus(trigram_assignments_filepath))\n",
    "test_corpus = list(read_corpus(trigram_assignments_filepath, tokens_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll instantiate a Doc2Vec model with a vector size with 300 words and iterating over the training corpus 100 times. We set the minimum word count to 5 in order to discard words with very few occurrences. (Without a variety of representative examples, retaining such infrequent words can often make a model worse!) Typical iteration counts in published 'Paragraph Vectors' results, using 10s-of-thousands to millions of docs, are 10-20. More iterations take more time and eventually reach a point of diminishing returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=300, min_count=5, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's build a vocabulary. Essentially, the vocabulary is a dictionary (accessible via `model.wv.vocab`) of all of the unique words extracted from the training corpus along with the count (e.g., `model.wv.vocab['penalty'].count` for counts for the word `penalty`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-12 00:11:42,369 : INFO : collecting all words and their counts\n",
      "2018-10-12 00:11:42,372 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2018-10-12 00:11:42,392 : INFO : collected 7207 word types and 179 unique tags from a corpus of 179 examples and 77911 words\n",
      "2018-10-12 00:11:42,394 : INFO : Loading a fresh vocabulary\n",
      "2018-10-12 00:11:42,405 : INFO : effective_min_count=5 retains 2128 unique words (29% of original 7207, drops 5079)\n",
      "2018-10-12 00:11:42,406 : INFO : effective_min_count=5 leaves 69681 word corpus (89% of original 77911, drops 8230)\n",
      "2018-10-12 00:11:42,414 : INFO : deleting the raw counts dictionary of 7207 items\n",
      "2018-10-12 00:11:42,416 : INFO : sample=0.001 downsamples 56 most-common words\n",
      "2018-10-12 00:11:42,422 : INFO : downsampling leaves estimated 61600 word corpus (88.4% of prior 69681)\n",
      "2018-10-12 00:11:42,434 : INFO : estimated required memory for 2128 words and 300 dimensions: 6421800 bytes\n",
      "2018-10-12 00:11:42,435 : INFO : resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "model.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train our model. It should take ~10 seconds to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-12 00:12:52,812 : INFO : training model with 3 workers on 2128 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-10-12 00:12:52,909 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:52,924 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:52,928 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:52,930 : INFO : EPOCH - 1 : training on 77911 raw words (61881 effective words) took 0.1s, 618051 effective words/s\n",
      "2018-10-12 00:12:53,027 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:53,028 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:53,034 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:53,035 : INFO : EPOCH - 2 : training on 77911 raw words (61737 effective words) took 0.1s, 624358 effective words/s\n",
      "2018-10-12 00:12:53,117 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:53,136 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:53,140 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:53,141 : INFO : EPOCH - 3 : training on 77911 raw words (61682 effective words) took 0.1s, 593047 effective words/s\n",
      "2018-10-12 00:12:53,217 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:53,238 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:53,240 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:53,240 : INFO : EPOCH - 4 : training on 77911 raw words (61769 effective words) took 0.1s, 648982 effective words/s\n",
      "2018-10-12 00:12:53,317 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:53,333 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:53,341 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:53,342 : INFO : EPOCH - 5 : training on 77911 raw words (61806 effective words) took 0.1s, 633734 effective words/s\n",
      "2018-10-12 00:12:53,421 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:53,436 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:53,437 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:53,438 : INFO : EPOCH - 6 : training on 77911 raw words (61740 effective words) took 0.1s, 670169 effective words/s\n",
      "2018-10-12 00:12:53,511 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:53,528 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:53,533 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:53,534 : INFO : EPOCH - 7 : training on 77911 raw words (61812 effective words) took 0.1s, 658596 effective words/s\n",
      "2018-10-12 00:12:53,606 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:53,616 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:53,622 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:53,622 : INFO : EPOCH - 8 : training on 77911 raw words (61697 effective words) took 0.1s, 727760 effective words/s\n",
      "2018-10-12 00:12:53,705 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:53,725 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:53,728 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:53,729 : INFO : EPOCH - 9 : training on 77911 raw words (61721 effective words) took 0.1s, 610650 effective words/s\n",
      "2018-10-12 00:12:53,798 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:53,821 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:53,822 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:53,823 : INFO : EPOCH - 10 : training on 77911 raw words (61768 effective words) took 0.1s, 695224 effective words/s\n",
      "2018-10-12 00:12:53,899 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:53,920 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:53,921 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:53,921 : INFO : EPOCH - 11 : training on 77911 raw words (61786 effective words) took 0.1s, 655523 effective words/s\n",
      "2018-10-12 00:12:53,999 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:54,016 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:54,021 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:54,022 : INFO : EPOCH - 12 : training on 77911 raw words (61821 effective words) took 0.1s, 627626 effective words/s\n",
      "2018-10-12 00:12:54,112 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:54,131 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:54,137 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:54,138 : INFO : EPOCH - 13 : training on 77911 raw words (61807 effective words) took 0.1s, 562351 effective words/s\n",
      "2018-10-12 00:12:54,219 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:54,240 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:54,243 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:54,243 : INFO : EPOCH - 14 : training on 77911 raw words (61753 effective words) took 0.1s, 597719 effective words/s\n",
      "2018-10-12 00:12:54,322 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:54,338 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:54,345 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:54,345 : INFO : EPOCH - 15 : training on 77911 raw words (61814 effective words) took 0.1s, 648266 effective words/s\n",
      "2018-10-12 00:12:54,421 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:54,444 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:54,448 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:54,449 : INFO : EPOCH - 16 : training on 77911 raw words (61767 effective words) took 0.1s, 610244 effective words/s\n",
      "2018-10-12 00:12:54,532 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:54,545 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:54,546 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:54,547 : INFO : EPOCH - 17 : training on 77911 raw words (61810 effective words) took 0.1s, 646181 effective words/s\n",
      "2018-10-12 00:12:54,626 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:54,639 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:54,644 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:54,644 : INFO : EPOCH - 18 : training on 77911 raw words (61806 effective words) took 0.1s, 644072 effective words/s\n",
      "2018-10-12 00:12:54,717 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:54,740 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:54,741 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:54,741 : INFO : EPOCH - 19 : training on 77911 raw words (61794 effective words) took 0.1s, 651912 effective words/s\n",
      "2018-10-12 00:12:54,816 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:54,835 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:54,838 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:54,839 : INFO : EPOCH - 20 : training on 77911 raw words (61776 effective words) took 0.1s, 667736 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-12 00:12:54,920 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:54,940 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:54,943 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:54,944 : INFO : EPOCH - 21 : training on 77911 raw words (61789 effective words) took 0.1s, 611655 effective words/s\n",
      "2018-10-12 00:12:55,020 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:55,040 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:55,042 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:55,043 : INFO : EPOCH - 22 : training on 77911 raw words (61799 effective words) took 0.1s, 647797 effective words/s\n",
      "2018-10-12 00:12:55,125 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:55,141 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:55,143 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:55,144 : INFO : EPOCH - 23 : training on 77911 raw words (61711 effective words) took 0.1s, 628660 effective words/s\n",
      "2018-10-12 00:12:55,208 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:55,227 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:55,230 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:55,231 : INFO : EPOCH - 24 : training on 77911 raw words (61708 effective words) took 0.1s, 734740 effective words/s\n",
      "2018-10-12 00:12:55,292 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:55,308 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:55,312 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:55,313 : INFO : EPOCH - 25 : training on 77911 raw words (61750 effective words) took 0.1s, 774227 effective words/s\n",
      "2018-10-12 00:12:55,375 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:55,388 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:55,389 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:55,390 : INFO : EPOCH - 26 : training on 77911 raw words (61800 effective words) took 0.1s, 814825 effective words/s\n",
      "2018-10-12 00:12:55,453 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:55,468 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:55,472 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:55,473 : INFO : EPOCH - 27 : training on 77911 raw words (61817 effective words) took 0.1s, 770963 effective words/s\n",
      "2018-10-12 00:12:55,543 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:55,554 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:55,557 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:55,558 : INFO : EPOCH - 28 : training on 77911 raw words (61707 effective words) took 0.1s, 759792 effective words/s\n",
      "2018-10-12 00:12:55,621 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:55,640 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:55,641 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:55,642 : INFO : EPOCH - 29 : training on 77911 raw words (61801 effective words) took 0.1s, 754422 effective words/s\n",
      "2018-10-12 00:12:55,711 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:55,724 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:55,726 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:55,727 : INFO : EPOCH - 30 : training on 77911 raw words (61865 effective words) took 0.1s, 754682 effective words/s\n",
      "2018-10-12 00:12:55,792 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:55,805 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:55,812 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:55,812 : INFO : EPOCH - 31 : training on 77911 raw words (61777 effective words) took 0.1s, 739354 effective words/s\n",
      "2018-10-12 00:12:55,879 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:55,896 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:55,896 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:55,897 : INFO : EPOCH - 32 : training on 77911 raw words (61863 effective words) took 0.1s, 762894 effective words/s\n",
      "2018-10-12 00:12:55,967 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:55,983 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:55,983 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:55,984 : INFO : EPOCH - 33 : training on 77911 raw words (61739 effective words) took 0.1s, 742014 effective words/s\n",
      "2018-10-12 00:12:56,050 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:56,062 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:56,067 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:56,068 : INFO : EPOCH - 34 : training on 77911 raw words (61834 effective words) took 0.1s, 754361 effective words/s\n",
      "2018-10-12 00:12:56,136 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:56,156 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:56,157 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:56,157 : INFO : EPOCH - 35 : training on 77911 raw words (61817 effective words) took 0.1s, 724237 effective words/s\n",
      "2018-10-12 00:12:56,229 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:56,245 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:56,249 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:56,249 : INFO : EPOCH - 36 : training on 77911 raw words (61793 effective words) took 0.1s, 685236 effective words/s\n",
      "2018-10-12 00:12:56,317 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:56,329 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:56,334 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:56,335 : INFO : EPOCH - 37 : training on 77911 raw words (61708 effective words) took 0.1s, 741519 effective words/s\n",
      "2018-10-12 00:12:56,398 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:56,414 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:56,417 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:56,418 : INFO : EPOCH - 38 : training on 77911 raw words (61891 effective words) took 0.1s, 774072 effective words/s\n",
      "2018-10-12 00:12:56,484 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:56,498 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:56,501 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:56,501 : INFO : EPOCH - 39 : training on 77911 raw words (61816 effective words) took 0.1s, 767232 effective words/s\n",
      "2018-10-12 00:12:56,566 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:56,587 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:56,588 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:56,589 : INFO : EPOCH - 40 : training on 77911 raw words (61847 effective words) took 0.1s, 746090 effective words/s\n",
      "2018-10-12 00:12:56,668 : INFO : worker thread finished; awaiting finish of 2 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-12 00:12:56,687 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:56,693 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:56,694 : INFO : EPOCH - 41 : training on 77911 raw words (61773 effective words) took 0.1s, 595553 effective words/s\n",
      "2018-10-12 00:12:56,785 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:56,807 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:56,809 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:56,810 : INFO : EPOCH - 42 : training on 77911 raw words (61795 effective words) took 0.1s, 549440 effective words/s\n",
      "2018-10-12 00:12:56,892 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:56,918 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:56,920 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:56,921 : INFO : EPOCH - 43 : training on 77911 raw words (61854 effective words) took 0.1s, 576826 effective words/s\n",
      "2018-10-12 00:12:57,011 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:57,025 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:57,036 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:57,037 : INFO : EPOCH - 44 : training on 77911 raw words (61812 effective words) took 0.1s, 556218 effective words/s\n",
      "2018-10-12 00:12:57,107 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:57,125 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:57,128 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:57,129 : INFO : EPOCH - 45 : training on 77911 raw words (61761 effective words) took 0.1s, 683172 effective words/s\n",
      "2018-10-12 00:12:57,192 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:57,206 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:57,208 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:57,208 : INFO : EPOCH - 46 : training on 77911 raw words (61722 effective words) took 0.1s, 809592 effective words/s\n",
      "2018-10-12 00:12:57,273 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:57,287 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:57,291 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:57,292 : INFO : EPOCH - 47 : training on 77911 raw words (61815 effective words) took 0.1s, 757544 effective words/s\n",
      "2018-10-12 00:12:57,358 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:57,367 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:57,370 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:57,371 : INFO : EPOCH - 48 : training on 77911 raw words (61792 effective words) took 0.1s, 810931 effective words/s\n",
      "2018-10-12 00:12:57,436 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:57,449 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:57,454 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:57,455 : INFO : EPOCH - 49 : training on 77911 raw words (61797 effective words) took 0.1s, 766435 effective words/s\n",
      "2018-10-12 00:12:57,521 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:57,534 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:57,536 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:57,537 : INFO : EPOCH - 50 : training on 77911 raw words (61731 effective words) took 0.1s, 773479 effective words/s\n",
      "2018-10-12 00:12:57,604 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:57,619 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:57,627 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:57,627 : INFO : EPOCH - 51 : training on 77911 raw words (61816 effective words) took 0.1s, 698415 effective words/s\n",
      "2018-10-12 00:12:57,699 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:57,717 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:57,718 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:57,719 : INFO : EPOCH - 52 : training on 77911 raw words (61762 effective words) took 0.1s, 694474 effective words/s\n",
      "2018-10-12 00:12:57,789 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:57,805 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:57,808 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:57,809 : INFO : EPOCH - 53 : training on 77911 raw words (61822 effective words) took 0.1s, 704522 effective words/s\n",
      "2018-10-12 00:12:57,876 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:57,891 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:57,891 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:57,892 : INFO : EPOCH - 54 : training on 77911 raw words (61853 effective words) took 0.1s, 762350 effective words/s\n",
      "2018-10-12 00:12:57,966 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:57,982 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:57,986 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:57,987 : INFO : EPOCH - 55 : training on 77911 raw words (61738 effective words) took 0.1s, 675488 effective words/s\n",
      "2018-10-12 00:12:58,053 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:58,071 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:58,074 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:58,075 : INFO : EPOCH - 56 : training on 77911 raw words (61830 effective words) took 0.1s, 740519 effective words/s\n",
      "2018-10-12 00:12:58,143 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:58,159 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:58,159 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:58,160 : INFO : EPOCH - 57 : training on 77911 raw words (61723 effective words) took 0.1s, 758322 effective words/s\n",
      "2018-10-12 00:12:58,224 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:58,240 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:58,242 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:58,242 : INFO : EPOCH - 58 : training on 77911 raw words (61820 effective words) took 0.1s, 770779 effective words/s\n",
      "2018-10-12 00:12:58,308 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:58,321 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:58,322 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:58,323 : INFO : EPOCH - 59 : training on 77911 raw words (61734 effective words) took 0.1s, 787884 effective words/s\n",
      "2018-10-12 00:12:58,388 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:58,402 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:58,403 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:58,404 : INFO : EPOCH - 60 : training on 77911 raw words (61858 effective words) took 0.1s, 792373 effective words/s\n",
      "2018-10-12 00:12:58,466 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:58,485 : INFO : worker thread finished; awaiting finish of 1 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-12 00:12:58,493 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:58,494 : INFO : EPOCH - 61 : training on 77911 raw words (61798 effective words) took 0.1s, 701348 effective words/s\n",
      "2018-10-12 00:12:58,561 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:58,573 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:58,580 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:58,580 : INFO : EPOCH - 62 : training on 77911 raw words (61779 effective words) took 0.1s, 732284 effective words/s\n",
      "2018-10-12 00:12:58,642 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:58,656 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:58,657 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:58,658 : INFO : EPOCH - 63 : training on 77911 raw words (61748 effective words) took 0.1s, 822852 effective words/s\n",
      "2018-10-12 00:12:58,723 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:58,740 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:58,746 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:58,747 : INFO : EPOCH - 64 : training on 77911 raw words (61743 effective words) took 0.1s, 715603 effective words/s\n",
      "2018-10-12 00:12:58,812 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:58,827 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:58,829 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:58,830 : INFO : EPOCH - 65 : training on 77911 raw words (61750 effective words) took 0.1s, 762376 effective words/s\n",
      "2018-10-12 00:12:58,893 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:58,905 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:58,907 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:58,908 : INFO : EPOCH - 66 : training on 77911 raw words (61803 effective words) took 0.1s, 828689 effective words/s\n",
      "2018-10-12 00:12:58,969 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:58,987 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:58,989 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:58,989 : INFO : EPOCH - 67 : training on 77911 raw words (61847 effective words) took 0.1s, 792734 effective words/s\n",
      "2018-10-12 00:12:59,059 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:59,070 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:59,072 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:59,073 : INFO : EPOCH - 68 : training on 77911 raw words (61824 effective words) took 0.1s, 761043 effective words/s\n",
      "2018-10-12 00:12:59,139 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:59,154 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:59,155 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:59,155 : INFO : EPOCH - 69 : training on 77911 raw words (61824 effective words) took 0.1s, 784062 effective words/s\n",
      "2018-10-12 00:12:59,218 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:59,232 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:59,237 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:59,238 : INFO : EPOCH - 70 : training on 77911 raw words (61850 effective words) took 0.1s, 775815 effective words/s\n",
      "2018-10-12 00:12:59,301 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:59,317 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:59,322 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:59,323 : INFO : EPOCH - 71 : training on 77911 raw words (61733 effective words) took 0.1s, 755947 effective words/s\n",
      "2018-10-12 00:12:59,386 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:59,403 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:59,405 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:59,406 : INFO : EPOCH - 72 : training on 77911 raw words (61786 effective words) took 0.1s, 767377 effective words/s\n",
      "2018-10-12 00:12:59,469 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:59,484 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:59,485 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:59,485 : INFO : EPOCH - 73 : training on 77911 raw words (61795 effective words) took 0.1s, 805058 effective words/s\n",
      "2018-10-12 00:12:59,542 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:59,558 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:59,560 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:59,561 : INFO : EPOCH - 74 : training on 77911 raw words (61766 effective words) took 0.1s, 835865 effective words/s\n",
      "2018-10-12 00:12:59,622 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:59,640 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:59,645 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:59,645 : INFO : EPOCH - 75 : training on 77911 raw words (61822 effective words) took 0.1s, 779309 effective words/s\n",
      "2018-10-12 00:12:59,711 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:59,725 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:59,727 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:59,728 : INFO : EPOCH - 76 : training on 77911 raw words (61793 effective words) took 0.1s, 767648 effective words/s\n",
      "2018-10-12 00:12:59,789 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:59,805 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:59,807 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:59,808 : INFO : EPOCH - 77 : training on 77911 raw words (61696 effective words) took 0.1s, 783803 effective words/s\n",
      "2018-10-12 00:12:59,871 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:59,885 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:59,886 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:59,887 : INFO : EPOCH - 78 : training on 77911 raw words (61714 effective words) took 0.1s, 795703 effective words/s\n",
      "2018-10-12 00:12:59,951 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:12:59,965 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:12:59,966 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:12:59,966 : INFO : EPOCH - 79 : training on 77911 raw words (61799 effective words) took 0.1s, 799532 effective words/s\n",
      "2018-10-12 00:13:00,032 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:13:00,049 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:13:00,052 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:13:00,052 : INFO : EPOCH - 80 : training on 77911 raw words (61757 effective words) took 0.1s, 734574 effective words/s\n",
      "2018-10-12 00:13:00,128 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:13:00,135 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:13:00,139 : INFO : worker thread finished; awaiting finish of 0 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-12 00:13:00,140 : INFO : EPOCH - 81 : training on 77911 raw words (61729 effective words) took 0.1s, 729104 effective words/s\n",
      "2018-10-12 00:13:00,205 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:13:00,218 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:13:00,222 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:13:00,223 : INFO : EPOCH - 82 : training on 77911 raw words (61794 effective words) took 0.1s, 757972 effective words/s\n",
      "2018-10-12 00:13:00,287 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:13:00,306 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:13:00,309 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:13:00,310 : INFO : EPOCH - 83 : training on 77911 raw words (61826 effective words) took 0.1s, 750085 effective words/s\n",
      "2018-10-12 00:13:00,375 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:13:00,388 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:13:00,392 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:13:00,392 : INFO : EPOCH - 84 : training on 77911 raw words (61835 effective words) took 0.1s, 770568 effective words/s\n",
      "2018-10-12 00:13:00,455 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:13:00,471 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:13:00,472 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:13:00,472 : INFO : EPOCH - 85 : training on 77911 raw words (61810 effective words) took 0.1s, 796797 effective words/s\n",
      "2018-10-12 00:13:00,532 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:13:00,548 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:13:00,549 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:13:00,550 : INFO : EPOCH - 86 : training on 77911 raw words (61683 effective words) took 0.1s, 841828 effective words/s\n",
      "2018-10-12 00:13:00,610 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:13:00,624 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:13:00,629 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:13:00,630 : INFO : EPOCH - 87 : training on 77911 raw words (61908 effective words) took 0.1s, 805616 effective words/s\n",
      "2018-10-12 00:13:00,689 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:13:00,702 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:13:00,707 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:13:00,708 : INFO : EPOCH - 88 : training on 77911 raw words (61897 effective words) took 0.1s, 817481 effective words/s\n",
      "2018-10-12 00:13:00,775 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:13:00,789 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:13:00,791 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:13:00,791 : INFO : EPOCH - 89 : training on 77911 raw words (61804 effective words) took 0.1s, 765149 effective words/s\n",
      "2018-10-12 00:13:00,852 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:13:00,866 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:13:00,871 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:13:00,872 : INFO : EPOCH - 90 : training on 77911 raw words (61730 effective words) took 0.1s, 807233 effective words/s\n",
      "2018-10-12 00:13:00,937 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:13:00,951 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:13:00,956 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:13:00,957 : INFO : EPOCH - 91 : training on 77911 raw words (61727 effective words) took 0.1s, 750700 effective words/s\n",
      "2018-10-12 00:13:01,022 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:13:01,035 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:13:01,038 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:13:01,038 : INFO : EPOCH - 92 : training on 77911 raw words (61793 effective words) took 0.1s, 800200 effective words/s\n",
      "2018-10-12 00:13:01,101 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:13:01,121 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:13:01,123 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:13:01,123 : INFO : EPOCH - 93 : training on 77911 raw words (61743 effective words) took 0.1s, 741363 effective words/s\n",
      "2018-10-12 00:13:01,190 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:13:01,201 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:13:01,205 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:13:01,205 : INFO : EPOCH - 94 : training on 77911 raw words (61666 effective words) took 0.1s, 790315 effective words/s\n",
      "2018-10-12 00:13:01,274 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:13:01,291 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:13:01,292 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:13:01,293 : INFO : EPOCH - 95 : training on 77911 raw words (61809 effective words) took 0.1s, 727553 effective words/s\n",
      "2018-10-12 00:13:01,359 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:13:01,375 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:13:01,376 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:13:01,376 : INFO : EPOCH - 96 : training on 77911 raw words (61688 effective words) took 0.1s, 771804 effective words/s\n",
      "2018-10-12 00:13:01,440 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:13:01,460 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:13:01,461 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:13:01,461 : INFO : EPOCH - 97 : training on 77911 raw words (61847 effective words) took 0.1s, 743943 effective words/s\n",
      "2018-10-12 00:13:01,528 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:13:01,544 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:13:01,548 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:13:01,549 : INFO : EPOCH - 98 : training on 77911 raw words (61806 effective words) took 0.1s, 727909 effective words/s\n",
      "2018-10-12 00:13:01,609 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:13:01,624 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:13:01,624 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:13:01,625 : INFO : EPOCH - 99 : training on 77911 raw words (61810 effective words) took 0.1s, 835514 effective words/s\n",
      "2018-10-12 00:13:01,687 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-12 00:13:01,702 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-12 00:13:01,704 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-12 00:13:01,705 : INFO : EPOCH - 100 : training on 77911 raw words (61810 effective words) took 0.1s, 787118 effective words/s\n",
      "2018-10-12 00:13:01,707 : INFO : training on a 7791100 raw words (6178529 effective words) took 8.9s, 694735 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.6 s, sys: 532 ms, total: 21.1 s\n",
      "Wall time: 8.9 s\n"
     ]
    }
   ],
   "source": [
    "%time model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-12 00:14:54,197 : INFO : saving Doc2Vec object under /Users/carlossouza/Dropbox/2018/OMSCS/CS-6460-Education-Technology/bettertogether/models/doc2vec_model_assignment_2, separately None\n",
      "2018-10-12 00:14:54,254 : INFO : saved /Users/carlossouza/Dropbox/2018/OMSCS/CS-6460-Education-Technology/bettertogether/models/doc2vec_model_assignment_2\n"
     ]
    }
   ],
   "source": [
    "model_path = os.path.join(os.getcwd(), \"models\")\n",
    "doc2vec_model_filepath = os.path.join(model_path, 'doc2vec_model_' + assignment_code)\n",
    "if save_files:\n",
    "    model.save(doc2vec_model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Test the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! Now, let's test the result by first looking who are the most similar students of **Carlos Souza**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-12 00:17:03,079 : INFO : precomputing L2-norms of doc weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar documents of Carlos Souza for Assignment 2\n",
      "0.3829500675201416 \tAlaa Shafaee\n",
      "0.3700012266635895 \tHector Ortiz-Mena\n",
      "0.3654715120792389 \tJoshua Harris\n",
      "0.3286665380001068 \tChristopher Bischke\n",
      "0.3188931345939636 \tHieu Nguyen\n",
      "0.3181723356246948 \tZhiyu Zong\n",
      "0.31330111622810364 \tDan Fujita\n",
      "0.2916083335876465 \tRui Zhan\n",
      "0.28721070289611816 \tMitchell Tufford\n",
      "0.279945433139801 \tLokesh Pathak\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "test_student = \"Carlos Souza\"\n",
    "\n",
    "similar_doc = model.docvecs.most_similar(test_student) \n",
    "print(\"Most similar documents of \" + test_student + \" for \" + assignment)\n",
    "for item in similar_doc:\n",
    "    print(str(item[1])  + \" \\t\" + str(item[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model says that, based on **Assignment 2**, **Carlos Souza**'s most similar student is **Alaa Shafaee**. Let's see if that's true by retrieving main topics from Carlos Souza's Assignment 2 and Alaa Shafaee's Assignment 2 and compare them. We will do it by using an NPL API called TextRazor.\n",
    "\n",
    "First, let's define a function `print_top_entities` to retrieve main topics of an assignment, and initialize TextRazor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_entities(student, limit=15):\n",
    "    text = textract.process(os.path.join(path, student + \" - \" + assignment + \".pdf\")).decode(\"utf-8\")\n",
    "    response = client.analyze(text)\n",
    "    entities = list(response.entities())\n",
    "    entities.sort(key=lambda x: x.relevance_score, reverse=True)\n",
    "    seen = set()\n",
    "    index = 0\n",
    "    for entity in entities:\n",
    "        if entity.id not in seen:\n",
    "            print(entity.id, entity.relevance_score, entity.confidence_score)\n",
    "            seen.add(entity.id)\n",
    "            index += 1\n",
    "            if index >= limit:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textrazor\n",
    "\n",
    "textrazor.api_key = \"d26228c7df9e0ea2a9c2656b520eefe049ae8c837fb1ef5a95619e04\"\n",
    "client = textrazor.TextRazor(extractors=[\"entities\", \"topics\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's compare Carlos Souza's main topics and Alaa Shafaee's main topics in Assignment 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intelligent tutoring system 1 8.865\n",
      "Social media 1 9.509\n",
      "Social networking service 1 1.756\n",
      "Metacognition 1 14.24\n",
      "Learning 0.9217 10.26\n",
      "Educational technology 0.8096 16.99\n",
      "Pedagogy 0.8096 5.95\n",
      "Learning environment 0.8002 2.443\n",
      "Problem solving 0.7723 6.813\n",
      "Motivation 0.7709 4.478\n",
      "Virtual community 0.7603 4.903\n",
      "Carol Dweck 0.7437 4.027\n",
      "Facebook 0.7412 15.26\n",
      "Community 0.7169 1.289\n",
      "Education 0.7148 5.371\n"
     ]
    }
   ],
   "source": [
    "print_top_entities(\"Carlos Souza\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intelligent tutoring system 1 8.568\n",
      "Metacognition 1 9.326\n",
      "Learning theory (education) 0.8641 4.046\n",
      "Cognitive tutor 0.8053 5.843\n",
      "Educational technology 0.7945 4.005\n",
      "Teaching method 0.7295 1.722\n",
      "Project-based learning 0.7283 9.479\n",
      "Problem-based learning 0.7254 12.69\n",
      "Constructionism (learning theory) 0.659 3.184\n",
      "Learning 0.6255 9.671\n",
      "Problem solving 0.6051 5.814\n",
      "Motivation 0.5837 5.01\n",
      "Simulation 0.5769 5.537\n",
      "Test (assessment) 0.5768 2.248\n",
      "Pedagogy 0.5468 6.151\n"
     ]
    }
   ],
   "source": [
    "print_top_entities(\"Alaa Shafaee\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, our algorithm worked perfectly! The API identifies all main topics and most of them are the same!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next steps\n",
    "\n",
    "- If you need support, drop me a note at [souza@gatech.edu](mailto:souza@gatech.edu), I will be glad to help.\n",
    "\n",
    "### Further readings\n",
    "\n",
    "- [Distributed Representations of Sentences and Documents](https://cs.stanford.edu/~quocle/paragraph_vector.pdf)\n",
    "- [Doc2Vec Tutorial on the Lee Dataset](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb)\n",
    "- [Modern NLP in Python](https://github.com/skipgram/modern-nlp-in-python/blob/master/executable/Modern_NLP_in_Python.ipynb)\n",
    "- [Gensim - Topic Modelling for Humans - tutorials](https://radimrehurek.com/gensim/tutorial.html)\n",
    "- [TextRazor API tutorials](https://www.textrazor.com/tutorials)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
